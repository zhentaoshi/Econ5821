{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning \n",
    "\n",
    "Zhentao Shi\n",
    "\n",
    "<img src=\"graph/Ada_Lovelace.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Machine learning and artificial intelligence:\n",
    "\n",
    "* Technology or alchemy?\n",
    "* Statistics or biology?\n",
    "* [Tom Sargent, (2019)](https://www.project-syndicate.org/commentary/artificial-intelligence-new-economic-models-by-thomas-j-sargent-2019-11)\n",
    "* Large language models\n",
    "* Generative models\n",
    "* The AI revolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* Connection between $X$ and $Y$\n",
    "* Regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A set of data fitting procedures focusing on out-of-sample prediction\n",
    "* Repeat a scientific experiment for $n$ times and obtain a dataset $(y_i, x_i)_{i=1}^n$.\n",
    "* How to best predict $y_{n+1}$ given $x_{n+1}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "* Only about $X$\n",
    "* Density estimation, principal component analysis, and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Conventional Statistics\n",
    "\n",
    "* Consistency\n",
    "* Asymptotic distribution (hopefully normal)\n",
    "* Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Learning's Responses\n",
    "\n",
    "* Efficiency is mostly irrelevant given big data\n",
    "* Statistical inference may not be the goal\n",
    "    * Recommendation system on Amazon or Taobao\n",
    "    * Care about the prediction accuracy, not the causal link\n",
    "* Is there a data generating process (DGP)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "# Load California Housing Dataset\n",
    "california = fetch_california_housing()\n",
    "df = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "df['MedHouseVal'] = california.target\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# display the summary statistics of the dataset df\n",
    "\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the problem\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Add a constant to the independent value\n",
    "X1 = sm.add_constant(X)\n",
    "\n",
    "# Make regression model \n",
    "model = sm.OLS(y, X1)\n",
    "\n",
    "# Fit model and print results\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X['Population'] = np.log(X['Population']) # log transformation of the population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "nobs = df.shape[0]\n",
    "# randomly draw a number from each column of X\n",
    "\n",
    "new_house = (X1.iloc[0]).copy() # randomly generate features of a new house\n",
    "\n",
    "for i in range(1, X1.shape[1]):\n",
    "    new_house[i] = X1.iloc[ random.randint(0, nobs),  i].copy()\n",
    "\n",
    "print(new_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# use new house to predict the price\n",
    "results.predict(exog = new_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised\n",
    "\n",
    "MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pareto\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Simulate a random dataset from a Pareto distribution\n",
    "n = 100\n",
    "alpha_true = 2\n",
    "data = pareto.rvs(alpha_true, size=n)\n",
    "# data = np.random.uniform(1.1, n) # a misspecified distribution\n",
    "\n",
    "# Define the log-likelihood function for Pareto distribution\n",
    "def pareto_log_likelihood(params, data):\n",
    "    alpha = params\n",
    "    log_likelihood = np.sum(np.log(alpha) - (1+alpha)*np.log(data) )\n",
    "    return -log_likelihood\n",
    "\n",
    "# Find the MLE estimate\n",
    "initial_guess = [1]\n",
    "result = minimize(pareto_log_likelihood, initial_guess, args=(data,))\n",
    "mle_alpha = result.x[0]\n",
    "\n",
    "# Print the MLE estimate\n",
    "print(\"Estimated alpha:\", mle_alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonparametric Estimation\n",
    "\n",
    "* *Parametric*: a finite number of parameters\n",
    "* *Nonparametric*: an infinite number of parameters\n",
    "\n",
    "* Some ideas in nonparametric estimation is directly related to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Density Estimation\n",
    "\n",
    "* Density estimation given a sample $(x_1,\\ldots,x_n)$\n",
    "* If drawn from a parametric family, MLE for estimation\n",
    "* Misspecification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Histogram is nonparametric\n",
    "    * If grid too fine, small bias but large variance\n",
    "    * If grid too coarse, small variance but large bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "\n",
    "n = 200\n",
    "x_base = np.arange(0.01, 1, 0.01)\n",
    "breaks_list = [4, 12, 60]\n",
    "\n",
    "fig, axs = plt.subplots(3, 3)\n",
    "fig.set_size_inches(8, 8)\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ii = i // 3\n",
    "    bb = breaks_list[i % 3]\n",
    "    x = np.random.beta(2, 2, size=n)\n",
    "    ax.hist(x, bins=bb, density=True)\n",
    "    ax.plot(x_base, beta.pdf(x_base, 2, 2), color=\"red\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_title(f\"Breaks={bb}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variance-Bias Tradeoff\n",
    "\n",
    "![](graph/bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Conditional Mean\n",
    "\n",
    "* Conditional mean $$f(x) = E[y_i |x_i = x]$$ given a sample $(y_i, x_i)$. \n",
    "* Solve \n",
    "$$\n",
    "\\min_f E[ (y_i - f(x_i) )^2 ]\n",
    "$$\n",
    "* In general $f(x)$ is a nonlinear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Conditional Mean\n",
    "\n",
    "* Conditional mean $$f(x) = E[y_i |x_i = x]$$ given a sample $(y_i, x_i)$. \n",
    "* Solve \n",
    "$$\n",
    "\\min_f E[ (y_i - f(x_i) )^2 ]\n",
    "$$\n",
    "* In general $f(x)$ is a nonlinear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Series Estimation\n",
    "\n",
    "* Series expansion to approximate $f(x)$\n",
    "* Generates many additive regressors\n",
    "    * Ex: bounded, continuous and differentiate function has a series\n",
    "representation $f(x) = \\sum_{k=0}^{\\infty} \\beta_k \\cos (\\frac{k}{2}\\pi x )$.\n",
    "    * In finite sample, choose a finite $K$, usually much smaller than $n$\n",
    "    * Asymptotically $K \\to \\infty$ as $n \\to \\infty$ so that\n",
    "$$\n",
    "f_K(x) = \\sum_{k=0}^{K} \\beta_k \\cos \\left(\\frac{k}{2}\\pi x \\right) \\to f(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Bias-variance trade-off\n",
    "    * Big $K$: small bias and large variance \n",
    "    * Small $K$: small variance and large bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Penalization\n",
    "\n",
    "* Specify a sufficiently large $K$, and then add a penalty term to control the complexity\n",
    "* Eg: *Ridge regression*: \n",
    "$$\n",
    "\\min_\\beta \\  \\frac{1}{2n}  \\sum_{i=1}^n \\left(y_i - \\sum_{k=0}^{K} \\beta_k f_k(x_i) \\right)^2\n",
    "+ \\lambda \\sum_{k=0}^K \\beta_k^2,\n",
    "$$\n",
    "where $\\lambda$ is the tuning parameter such that $\\lambda \\to 0$ as $n\\to \\infty$, and\n",
    "$f_k(x_i) = \\cos \\left(\\frac{k}{2}\\pi x_i \\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In compact notation, let $Y=(y_1,\\ldots,y_n)'$ and\n",
    "$X = (X_{ik} = f_k(x_i) )$, the above problem can be written as\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\Vert \\beta \\Vert_2 ^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tuning Parameter\n",
    "\n",
    "* *Information criterion*: AIC, BIC\n",
    "* *Cross validation*\n",
    "\n",
    "\n",
    "* Active statistical research, but has little economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Econometrics Workflow\n",
    "\n",
    "![](graph/metric_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "![ ](graph/ML_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "\n",
    "* Machine learning's main purpose is often prediction\n",
    "* Agnostic about the DGP.\n",
    "* Models are measured by their performance in prediction.\n",
    "* Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Training dataset\n",
    "* Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Testing sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `scikit-learn`\n",
    "\n",
    "* Python modules: a framework for many machine learning methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Validation (cross sectional data)\n",
    "\n",
    "* $S$-fold cross validation partitions the dataset into $S$ disjoint sections\n",
    "* Each iteration picks one of the sections as the (quasi) validation sample\n",
    "* The other $S-1$ sections as the training sample.\n",
    "* Compute an out-of-sample goodness-of-fit measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goodness of Fit (Out of Sample)\n",
    "\n",
    "* *Mean-squared prediction error* ${n_v}^{-1} \\sum_{i \\in val} (y_i - \\hat{y}_i)^2$ where $val$ is the validation set and $n_v$ is its cardinality, \n",
    "* *Mean-absolute prediction error* ${n_v}^{-1}\\sum_{i \\in val} |y_i - \\hat{y}_i|$. \n",
    "* *Out of sample R-squared* (OOS $R^2$):\n",
    "\n",
    "$$\n",
    "1 - \\frac{{n_v}^{-1} \\sum_{i \\in val} (y_i - \\hat{y}_i)^2}{{n_v}^{-1} \\sum_{i \\in val} y_i^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Repeat this process for $S$ times so that each of the $S$ sections are treated as the validation sample, \n",
    "* Average the goodness-of-fit measurement over the $S$ sections to determined the best tuning parameter. \n",
    "* In practice we can use  $S=5$ for 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Validation (time series data)\n",
    "\n",
    "* In time series context, cross validation must preserve the dependence structure. \n",
    "* If the time series is stationary, we can partition the data into $S$ consecutive blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "(i will skip this slide)\n",
    "\n",
    "* If the purpose is forecasting, then we can use nested CV. \n",
    "![ ](graph/CV_Figure.png)\n",
    "\n",
    "* Nested CV with fixed-length rolling window scheme\n",
    "* The sub-training data can also be an extending rolling window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection\n",
    "\n",
    "* Number of covariates $x_i$ can be large.\n",
    "\n",
    "* Conventional attitude: prior knowledge\n",
    "* Recently economists wake up from the long lasting negligence.\n",
    "    * Stock and Watson (2012): forecasting 143 US macroeconomic indicators.\n",
    "    * A horse race of several variable selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lasso\n",
    "\n",
    "* least-absolute-shrinkage-and-selection-operator\n",
    "(Lasso) (Tibshirani, 1996)\n",
    "* Penalizes the $L_1$ norm of the coefficients.\n",
    "The criterion function of Lasso is written as\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\Vert \\beta \\Vert_1\n",
    "$$\n",
    "where $\\lambda \\geq 0$ is a tuning parameter. \n",
    "\n",
    "Lasso shrinks some coefficients exactly to 0, in a wide range of values of $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![ ](graph/lasso_regression2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SCAD\n",
    "\n",
    "* Smoothly-clipped-absolute-deviation (SCAD) Fan and Li (2001):\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\sum_{j=1}^d \\rho_{\\lambda}( |\\beta_j| )\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\rho_{\\lambda}^{\\prime} (\\theta) = \\lambda \\left\\{ 1\\{\\theta\\leq \\lambda \\} +\n",
    "\\frac{(a\\lambda - \\theta)_+}{(a-1)\\lambda} \\cdot 1 \\{\\theta > \\lambda\\} \\right\\}\n",
    "$$\n",
    "for some $a>2$ and $\\theta>0$. \n",
    "\n",
    "* SCAD enjoys *oracle property*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adaptive Lasso\n",
    "\n",
    "*Adaptive Lasso* (Zou, 2006) also enjoys the oracle property.\n",
    "\n",
    "Two-step algorithm:\n",
    "1. First run a Lasso or ridge regression and save the estimator $\\hat{\\beta}^{(1)}$\n",
    "2. Solve \n",
    "$(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\sum_{j=1}^d  w_j |\\beta_j|$\n",
    "where $w_j = 1 /  |\\hat{\\beta}_j^{(1)} |^a$ and $a\\geq 1$ is a constant. (Common choice is $a = 1$ or 2).\n",
    "\n",
    "* Lee, Shi and Gao (2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### There are no packages for adaptive lasso in python like \"glmnet\" in R\n",
    "\n",
    "We can only use CVXPY to DIY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import pinv\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "# Set up the data\n",
    "n = 40\n",
    "p = 50\n",
    "b0 = np.concatenate([np.ones(10), np.zeros(p - 10)])\n",
    "x = np.random.normal(size=(n, p))\n",
    "y = np.dot(x, b0) + np.random.normal(size=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OLS\n",
    "ols = np.dot(pinv(np.dot(x.T, x)), np.dot(x.T, y))\n",
    "\n",
    "# Lasso\n",
    "lasso_cv = LassoCV(cv=5).fit(x, y)\n",
    "lasso_result = Lasso(alpha=lasso_cv.alpha_).fit(x, y)\n",
    "\n",
    "lasso_result.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stagewise Forward Selection\n",
    "\n",
    "More methods are available if prediction of the response variables is the sole purpose of the regression.\n",
    "\n",
    "Eg: *stagewise forward selection*\n",
    "\n",
    "1. Start from an empty model. \n",
    "2. Given many candidate $x_j$, in each round we add the regressor that can\n",
    "produce the biggest $R^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Close to the idea of *$L_2$ componentwise boosting*\n",
    "which does not adjust the coefficients fitted earlier\n",
    "\n",
    "* Shi and Huang (2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Second Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction-Oriented Methods\n",
    "\n",
    "* Methods that induces data-driven interaction of the covariates.\n",
    "* Interaction makes the covariates much more flexible\n",
    "* Insufficient theoretical understanding\n",
    "* \"Black-boxes\" methods\n",
    "\n",
    "* Surprisingly superior performance\n",
    "* Industry insiders are pondering \"alchemy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
    "\n",
    "# Create adaboost classifer object\n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=0.1)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression Tree\n",
    "\n",
    "* Supervised learning: $x \\to y $\n",
    "* Regression tree (Breiman, 1984) recursively partitions the space of the regressors\n",
    "    * Each time a covariate is split into two dummies\n",
    "    * Splitting criterion is aggressive reduction of the SSR\n",
    "    * Tuning parameter is the depth of the tree\n",
    "    * Given a dataset $d$ and the depth of the tree, the fitted tree $\\hat{r}(d)$ is deterministic\n",
    "\n",
    "- Example: Using longitude and latitude for Beijing housing price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging\n",
    "\n",
    "* Tree is unstable\n",
    "* *Bootstrap averaging*, or *bagging*, reduces variance of trees (Breiman, 1996)\n",
    "    * Grow a tree for each bootstrap sample\n",
    "    * Simple average\n",
    "\n",
    "* An example of the *ensemble learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Inoue and Kilian (2008): an early application of bagging in time series forecast.\n",
    "* Hirano and Wright (2017): a theoretical perspective on the risk reduction of bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "* *Random forest* (Breiman, 2001):\n",
    "    * Draw a bootstrap sample\n",
    "    * Before each split, shakes up the regressors by randomly sampling $m$ out of the total $p$ covarites. Stop until the depth of the tree is reached.\n",
    "    * Average the trees over the bootstrap samples\n",
    "    \n",
    "* The tuning parameters are the tree depth and $m$\n",
    "* More stable than bagging thanks to \"de-correlation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load California housing dataset\n",
    "california = fetch_california_housing()\n",
    "X, y = california.data, california.target\n",
    "feature_names = california.feature_names\n",
    "\n",
    "# training Sample with 300 observations\n",
    "train_indices = np.random.choice(np.arange(len(y)), size=300, replace=False)\n",
    "\n",
    "# Fit Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=500, random_state=101)\n",
    "# document: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# \"n_estimators\" is the number of trees in a forest\n",
    "\n",
    "rf.fit(X[train_indices], y[train_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot feature importances\n",
    "feat_importances = rf.feature_importances_\n",
    "indices = np.argsort(feat_importances)[::-1]\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), feat_importances[indices])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Consistency of random forest is not proved\n",
    "until Scornet, Biau, and Vert (2015)\n",
    "* Inferential theory was first established by\n",
    "Wager Athey (2018)  in the context of treatment effect estimation\n",
    "* Athey, Tibshirani, and Wager (2019) generalizes CART to local maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "* Bagging and random forest use equal weight on each generated tree for the ensemble\n",
    "* Tree boosting takes a deterministic approach for the weights\n",
    "    1. Use the original data $d^0=(x_i,y_i)$ to grow a shallow tree $\\hat{r}^{0}(d^0)$. Save the prediction $f^0_i = \\alpha \\cdot \\hat{r}^0 (d^0, x_i)$ where\n",
    "   $\\alpha\\in [0,1]$ is a shrinkage tuning parameter. Save\n",
    "   the residual $e_i^{0} = y_i - f^0_i$. Set $m=1$.\n",
    "    2. In the $m$-th iteration, use the data $d^m = (x_i,e_i^{m-1})$ to grow a shallow tree $\\hat{r}^{m}(d^m)$. Save the prediction $f^m_i =  f^{m-1}_i +  \\alpha \\cdot \\hat{r}^m (d, x_i)$. Save\n",
    "   the residual $e_i^{m} = y_i - f^m_i$. Update $m = m+1$.\n",
    "    3. Repeat Step 2 until $m > M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Boosting has three tuning parameters: the tree depth,  the shrinkage level $\\alpha$, and the number of iterations $M$\n",
    "* The algorithm can be sensitive to any of the three tuning parameters\n",
    "* When a model is tuned well, it can performs remarkably\n",
    "    * Example: Beijing housing data.\n",
    "    * Gradient boosting via the package `gbm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lianjia = pd.read_csv(\"data_example/lianjia.csv\",encoding='gbk')\n",
    "\n",
    "# Sampling with a seed for reproducibility\n",
    "lianjia = lianjia.sample(frac=0.05, random_state=1)\n",
    "\n",
    "train_data, test_data = train_test_split(lianjia, test_size=0.7, random_state=1)\n",
    "\n",
    "train_ind = train_data.index\n",
    "\n",
    "# Define the hyperparameters\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 40],  # equivalent to interaction.depth in R's gbm\n",
    "    'n_estimators': [1000, 5000, 9000],  # equivalent to n.trees\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1],  # equivalent to shrinkage\n",
    "    'min_samples_leaf': [10, 15, 20],  # equivalent to n.minobsinnode\n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predictors = [\"square\", \"livingRoom\", \"drawingRoom\", \"kitchen\", \"bathRoom\",\n",
    "              \"floor_total\", \"elevator\", \"ladderRatio\",\n",
    "              \"age\", \"DOM\", \"followers\", \"fiveYearsProperty\",\n",
    "              \"subway\", \"district\", \"Lng\", \"Lat\", \"t_trade\",\n",
    "              \"communityAverage\"]\n",
    "\n",
    "# Your target variable\n",
    "target = 'price'\n",
    "\n",
    "# Prepare your predictor and target datasets\n",
    "X = lianjia[predictors]\n",
    "y = lianjia[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the Gradient Boosting Regressor\n",
    "gbm = GradientBoostingRegressor(loss='squared_error', random_state=0)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator = gbm, \n",
    "                           param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_squared_error', \n",
    "                           cv=5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the time taken\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", -grid_search.best_score_)\n",
    "print(\"Time taken:\", duration, \"seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Get the best parameters from grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Set the best parameters to the new model\n",
    "gbm.set_params(**best_params)\n",
    "\n",
    "# Now when you call fit on gbm, it will use the best parameters\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# Predict with GBM model\n",
    "pred_boosting = gbm.predict(X_test)\n",
    "\n",
    "# Fit the linear regression model\n",
    "lmReg = LinearRegression()\n",
    "lmReg.fit(X_train, y_train)\n",
    "\n",
    "# Predict with linear regression model\n",
    "pred_lm = lmReg.predict(X_test)\n",
    "\n",
    "# Comparison\n",
    "r_squared_gbm = r2_score(y_test, pred_boosting)\n",
    "r_squared_lm = r2_score(y_test, pred_lm)\n",
    "\n",
    "print(\"R-squared of GBM prediction =\", r_squared_gbm)\n",
    "print(\"R-squared of LM prediction =\", r_squared_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Many variants of boosting algorithms\n",
    "    * $L_2$-boosting\n",
    "    * componentwise boosting\n",
    "    * AdaBoosting, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "Use SGD in the PPMLE\n",
    "* sample size 100,000\n",
    "* the number of parameters 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson.loglik = function( b, y, X ) {\n",
    "  b = as.matrix( b )\n",
    "  lambda =  exp( X %*% b )\n",
    "  ell = -mean( -lambda + y *  log(lambda) )\n",
    "  return(ell)\n",
    "}\n",
    "\n",
    "\n",
    "poisson.loglik.grad = function( b, y, X ) {\n",
    "  b = as.matrix( b )\n",
    "  lambda =  as.vector( exp( X %*% b ) )\n",
    "  ell = -colMeans( -lambda * X + y * X )\n",
    "  ell_eta = ell\n",
    "  return(ell_eta)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### generate the artificial data\n",
    "set.seed(898)\n",
    "nn = 1e5; K = 100\n",
    "\n",
    "X = cbind(1, matrix( runif( nn*(K-1) ), ncol = K-1 ) )\n",
    "b0 = rep(1, K) / K\n",
    "y = rpois(nn, exp( X %*% b0 ) )\n",
    "\n",
    "\n",
    "b.init = runif(K); b.init  = 2 * b.init / sum(b.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and these tuning parameters are related to N and K\n",
    "\n",
    "n = length(y)\n",
    "test_ind = sample(1:n, round(0.2*n) ) \n",
    "\n",
    "# 80% training data\n",
    "# 20% testing data\n",
    "\n",
    "y_test = y[test_ind]\n",
    "X_test = X[test_ind, ]\n",
    "\n",
    "y_train = y[-test_ind ]\n",
    "X_train = X[-test_ind, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "\n",
    "# sgd depends on\n",
    "# eta: the learning rate\n",
    "# epoch: the averaging small batch\n",
    "# the initial value\n",
    "\n",
    "set.seed(105)\n",
    "\n",
    "max_iter = 5000\n",
    "min_iter = 20\n",
    "eta=0.01\n",
    "epoch = round( 100*sqrt(K) ) # in this case K = 100\n",
    "\n",
    "b_old = b.init\n",
    "\n",
    "pts0 = Sys.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the iteration of gradient\n",
    "# In my experiment in March 2022, it takes 8 minutes\n",
    "\n",
    "for (i in 1:max_iter ){\n",
    "\n",
    "  loglik_old = poisson.loglik(b_old, y_train, X_train)\n",
    "  i_sample = sample(1:length(y_train), epoch, replace = TRUE )\n",
    "\n",
    "  # iteration formula\n",
    "  b_new = b_old - eta * poisson.loglik.grad(b_old, y_train[i_sample], X_train[i_sample, ])\n",
    "\n",
    "  loglik_new = poisson.loglik(b_new, y_test, X_test)\n",
    "  b_old = b_new # update\n",
    "\n",
    "  criterion =  loglik_old - loglik_new  \n",
    "  if (  criterion < 0.0001 & i >= min_iter ) break\n",
    "}\n",
    "\n",
    "cat(\"point estimate =\", b_new, \", log_lik = \", loglik_new, \"\\n\")\n",
    "pts1 = Sys.time( ) - pts0\n",
    "print(pts1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimx is too slow for this dataset.\n",
    "# Nelder-Mead method is too slow for this dataset\n",
    "\n",
    "# thus we only sgd with NLoptr\n",
    "\n",
    "# this model has only one layer\n",
    "# and the objective function is known to be convex\n",
    "# thus BFGS works well\n",
    "\n",
    "opts = list(\"algorithm\"=\"NLOPT_LD_SLSQP\",\"xtol_rel\"=1.0e-7, maxeval = 5000)\n",
    "\n",
    "pts0 = Sys.time( )\n",
    "res_BFGS = nloptr::nloptr( x0=b.init,\n",
    "                 eval_f=poisson.loglik,\n",
    "                 eval_grad_f = poisson.loglik.grad,\n",
    "                 opts=opts,\n",
    "                 y = y_train, X = X_train)\n",
    "print( res_BFGS )\n",
    "pts1 = Sys.time( ) - pts0\n",
    "print(pts1)\n",
    "\n",
    "b_hat_nlopt = res_BFGS$solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluation in the test sample\n",
    "cat(\"\\n\\n\\n\\n\\n\\n\\n\")\n",
    "cat(\"log lik in test data by sgd = \", poisson.loglik(b_new, y = y_test, X_test), \"\\n\")\n",
    "cat(\"log lik in test data by nlopt = \", poisson.loglik(b_hat_nlopt, y = y_test, X_test), \"\\n\")\n",
    "cat(\"log lik in test data by oracle = \", poisson.loglik(b0, y = y_test, X_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* Mature algorithms for implementation\n",
    "* Theoretical investigation is in progress\n",
    "* Economic applications are emerging"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
