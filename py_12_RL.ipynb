{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Reinforcement Learning in R\n",
    "\n",
    "This notebook builds a minimal reinforcement learning (RL) example using **Q-learning** in R.\n",
    "\n",
    "Goal: learn a policy that moves an agent to a target state as efficiently as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We use a simple 1D world with 5 states: `1, 2, 3, 4, 5`.\n",
    "\n",
    "- Start state: `1`\n",
    "- Terminal (goal) state: `5`\n",
    "- Actions: `left`, `right`\n",
    "- Transition: deterministic (left decreases state by 1; right increases by 1, bounded in `[1, 5]`)\n",
    "- Reward: `+1` when reaching state `5`, otherwise \"left\" gains `-0.02` and \"right\" gains `-0.04`\n",
    "\n",
    "This setting is tiny but enough to illustrate the RL ingredients: states, actions, rewards, and policy learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Environment definition\n",
    "n_states <- 5\n",
    "goal_state <- 5\n",
    "actions <- c(\"left\", \"right\")\n",
    "\n",
    "# Transition and reward function\n",
    "step_env <- function(state, action) {\n",
    "  if (action == \"left\") {\n",
    "    next_state <- max(1, state - 1)\n",
    "  } else if (action == \"right\") {\n",
    "    next_state <- min(n_states, state + 1)\n",
    "  }\n",
    "\n",
    "  done <- (next_state == goal_state)\n",
    "  reward <- if (done) 1 else if (action == \"left\") -0.02 else -0.04\n",
    "\n",
    "  return ( list(next_state = next_state, reward = reward, done = done ) )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy and Learning Rule\n",
    "\n",
    "We store action values in a Q-table `Q[state, action]`.\n",
    "\n",
    "- **Behavior policy (for training):** epsilon-greedy\n",
    "  - with probability `epsilon`, choose a random action (explore)\n",
    "  - otherwise choose the action with highest current Q-value (exploit)\n",
    "- **Update rule (Q-learning):**\n",
    "\n",
    "$$Q(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s',a')\\right]$$\n",
    "\n",
    "where `alpha` is the learning rate and `gamma` is the discount factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65793219",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# alpha <- 1.0 # macro uses alpha = 1.0\n",
    "alpha <- 0.8 # statistic practice\n",
    "gamma <- 0.9\n",
    "epsilon <- 0.2\n",
    "n_episodes <- 20\n",
    "max_steps <- 10\n",
    "\n",
    "\n",
    "# Q is a 5-by-2 matrix\n",
    "# Q-table: rows are states, columns are actions\n",
    "Q <- matrix(0, nrow = n_states, ncol = length(actions),\n",
    "            dimnames = list(state = 1:n_states, action = actions))\n",
    "\n",
    "# mapping from (state, Q) -> action\n",
    "choose_action <- function(state, Q, epsilon) {\n",
    "  if (runif(1) < epsilon) { # exploration\n",
    "    sample(actions, 1) # action = (left, right)\n",
    "  } else { # exploitation\n",
    "    actions[ which.max( Q[state, ] ) ]\n",
    "    # give \"state\", take the action that maximize \n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d876a",
   "metadata": {},
   "source": [
    "The matrix `Q` will evolve over episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "1446d735",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     action\n",
      "state    left right\n",
      "    1 -0.0542 -0.04\n",
      "    2 -0.0560 -0.04\n",
      "    3 -0.0542  0.00\n",
      "    4  0.0000  0.00\n",
      "    5  0.0000  0.00\n",
      "     action\n",
      "state    left  right\n",
      "    1 -0.0542 -0.076\n",
      "    2 -0.0560 -0.040\n",
      "    3 -0.0542 -0.040\n",
      "    4 -0.0560  1.000\n",
      "    5  0.0000  0.000\n",
      "     action\n",
      "state      left  right\n",
      "    1 -0.081902 -0.076\n",
      "    2 -0.056000 -0.076\n",
      "    3 -0.054200  0.860\n",
      "    4 -0.056000  1.000\n",
      "    5  0.000000  0.000\n",
      "     action\n",
      "state       left   right\n",
      "    1 -0.0937118 -0.1084\n",
      "    2 -0.0937118  0.7340\n",
      "    3 -0.0542000  0.8600\n",
      "    4 -0.0560000  1.0000\n",
      "    5  0.0000000  0.0000\n",
      "     action\n",
      "state       left  right\n",
      "    1 -0.1139066 0.6206\n",
      "    2 -0.0937118 0.7340\n",
      "    3 -0.0542000 0.8600\n",
      "    4 -0.0560000 1.0000\n",
      "    5  0.0000000 0.0000\n",
      "     action\n",
      "state       left  right\n",
      "    1 -0.1139066 0.6206\n",
      "    2 -0.0937118 0.7340\n",
      "    3  0.6406000 0.8600\n",
      "    4 -0.0560000 1.0000\n",
      "    5  0.0000000 0.0000\n",
      "     action\n",
      "state       left  right\n",
      "    1 -0.1139066 0.6206\n",
      "    2 -0.0937118 0.7340\n",
      "    3  0.6406000 0.8600\n",
      "    4 -0.0560000 1.0000\n",
      "    5  0.0000000 0.0000\n",
      "     action\n",
      "state       left  right\n",
      "    1 -0.1139066 0.6206\n",
      "    2 -0.0937118 0.7340\n",
      "    3  0.6406000 0.8600\n",
      "    4 -0.0560000 1.0000\n",
      "    5  0.0000000 0.0000\n",
      "     action\n",
      "state       left  right\n",
      "    1 -0.1139066 0.6206\n",
      "    2 -0.0937118 0.7340\n",
      "    3  0.6406000 0.8600\n",
      "    4 -0.0560000 1.0000\n",
      "    5  0.0000000 0.0000\n",
      "     action\n",
      "state       left  right\n",
      "    1 -0.1139066 0.6206\n",
      "    2 -0.0937118 0.7340\n",
      "    3  0.6406000 0.8600\n",
      "    4 -0.0560000 1.0000\n",
      "    5  0.0000000 0.0000\n",
      "     action\n",
      "state       left  right\n",
      "    1 -0.1139066 0.6206\n",
      "    2 -0.0937118 0.7340\n",
      "    3  0.6406000 0.8600\n",
      "    4 -0.0560000 1.0000\n",
      "    5  0.0000000 0.0000\n",
      "     action\n",
      "state       left  right\n",
      "    1 -0.1139066 0.6206\n",
      "    2 -0.0937118 0.7340\n",
      "    3  0.6406000 0.8600\n",
      "    4 -0.0560000 1.0000\n",
      "    5  0.0000000 0.0000\n",
      "     action\n",
      "state       left  right\n",
      "    1  0.5385400 0.6206\n",
      "    2 -0.0937118 0.7340\n",
      "    3  0.6406000 0.8600\n",
      "    4  0.7540000 1.0000\n",
      "    5  0.0000000 0.0000\n",
      "     action\n",
      "state       left  right\n",
      "    1  0.5385400 0.6206\n",
      "    2 -0.0937118 0.7340\n",
      "    3  0.6406000 0.8600\n",
      "    4  0.7540000 1.0000\n",
      "    5  0.0000000 0.0000\n",
      "     action\n",
      "state    left  right\n",
      "    1 0.53854 0.6206\n",
      "    2 0.53854 0.7340\n",
      "    3 0.64060 0.8600\n",
      "    4 0.75400 1.0000\n",
      "    5 0.00000 0.0000\n",
      "     action\n",
      "state    left  right\n",
      "    1 0.53854 0.6206\n",
      "    2 0.53854 0.7340\n",
      "    3 0.64060 0.8600\n",
      "    4 0.75400 1.0000\n",
      "    5 0.00000 0.0000\n",
      "     action\n",
      "state    left  right\n",
      "    1 0.53854 0.6206\n",
      "    2 0.53854 0.7340\n",
      "    3 0.64060 0.8600\n",
      "    4 0.75400 1.0000\n",
      "    5 0.00000 0.0000\n",
      "     action\n",
      "state    left  right\n",
      "    1 0.53854 0.6206\n",
      "    2 0.53854 0.7340\n",
      "    3 0.64060 0.8600\n",
      "    4 0.75400 1.0000\n",
      "    5 0.00000 0.0000\n",
      "     action\n",
      "state    left  right\n",
      "    1 0.53854 0.6206\n",
      "    2 0.53854 0.7340\n",
      "    3 0.64060 0.8600\n",
      "    4 0.75400 1.0000\n",
      "    5 0.00000 0.0000\n",
      "     action\n",
      "state    left  right\n",
      "    1 0.53854 0.6206\n",
      "    2 0.53854 0.7340\n",
      "    3 0.64060 0.8600\n",
      "    4 0.75400 1.0000\n",
      "    5 0.00000 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "episode_returns <- numeric(n_episodes)\n",
    "QQ <- matrix(0, n_states, n_episodes)\n",
    "\n",
    "for (ep in 1:n_episodes) {\n",
    "  state <- 1 # start with state == 1\n",
    "\n",
    "  for (t in 1:max_steps) { # 30 steps of maximum\n",
    "\n",
    "    action <- choose_action(state, Q, epsilon) # exploration vs exploitation\n",
    "    out <- step_env(state, action) \n",
    "    # outcome given the state and action in this step\n",
    "\n",
    "    s_next <- out$next_state\n",
    "    r <- out$reward\n",
    "    done <- out$done # collect info from outcome\n",
    "\n",
    "    # if (ep == n_episodes ) \n",
    "    # cat(\"step\", t, \"with reward\", r, \"with state\", s_next, \"\\n\") # report\n",
    "\n",
    "    a_idx <- match(action, actions) # action id\n",
    "    target <- r + gamma * max(Q[s_next, ])\n",
    "    # this period reward + pervious max over actions\n",
    "\n",
    "    Q[state, a_idx] <- (1 - alpha) * Q[state, a_idx] + alpha * target\n",
    "    # update either left or right of Q\n",
    "    # Q is updated in every step\n",
    "\n",
    "    state <- s_next\n",
    "\n",
    "    if (done) break\n",
    "  }\n",
    "  print(Q)\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Learned Policy\n",
    "\n",
    "After training, the greedy policy picks the action with the largest Q-value in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 5 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>state</th><th scope=col>action</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>right</td></tr>\n",
       "\t<tr><td>2</td><td>right</td></tr>\n",
       "\t<tr><td>3</td><td>right</td></tr>\n",
       "\t<tr><td>4</td><td>right</td></tr>\n",
       "\t<tr><td>5</td><td>left </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 5 × 2\n",
       "\\begin{tabular}{ll}\n",
       " state & action\\\\\n",
       " <int> & <chr>\\\\\n",
       "\\hline\n",
       "\t 1 & right\\\\\n",
       "\t 2 & right\\\\\n",
       "\t 3 & right\\\\\n",
       "\t 4 & right\\\\\n",
       "\t 5 & left \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 5 × 2\n",
       "\n",
       "| state &lt;int&gt; | action &lt;chr&gt; |\n",
       "|---|---|\n",
       "| 1 | right |\n",
       "| 2 | right |\n",
       "| 3 | right |\n",
       "| 4 | right |\n",
       "| 5 | left  |\n",
       "\n"
      ],
      "text/plain": [
       "  state action\n",
       "1 1     right \n",
       "2 2     right \n",
       "3 3     right \n",
       "4 4     right \n",
       "5 5     left  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "greedy_policy <- apply(Q, 1, function(q_row) actions[which.max(q_row)])\n",
    "\n",
    "policy_table <- data.frame(\n",
    "  state = 1:n_states,\n",
    "  action = greedy_policy,\n",
    "  row.names = NULL\n",
    ")\n",
    "\n",
    "policy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Greedy Episode (Evaluation)\n",
    "\n",
    "Now we run one episode using the learned greedy policy (`epsilon = 0`) to see the behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 5 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>step</th><th scope=col>state</th><th scope=col>action</th><th scope=col>reward</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0</td><td>1</td><td>NA   </td><td> 0.00</td></tr>\n",
       "\t<tr><td>1</td><td>2</td><td>right</td><td>-0.04</td></tr>\n",
       "\t<tr><td>2</td><td>3</td><td>right</td><td>-0.04</td></tr>\n",
       "\t<tr><td>3</td><td>4</td><td>right</td><td>-0.04</td></tr>\n",
       "\t<tr><td>4</td><td>5</td><td>right</td><td> 1.00</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 5 × 4\n",
       "\\begin{tabular}{llll}\n",
       " step & state & action & reward\\\\\n",
       " <dbl> & <dbl> & <chr> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 0 & 1 & NA    &  0.00\\\\\n",
       "\t 1 & 2 & right & -0.04\\\\\n",
       "\t 2 & 3 & right & -0.04\\\\\n",
       "\t 3 & 4 & right & -0.04\\\\\n",
       "\t 4 & 5 & right &  1.00\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 5 × 4\n",
       "\n",
       "| step &lt;dbl&gt; | state &lt;dbl&gt; | action &lt;chr&gt; | reward &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| 0 | 1 | NA    |  0.00 |\n",
       "| 1 | 2 | right | -0.04 |\n",
       "| 2 | 3 | right | -0.04 |\n",
       "| 3 | 4 | right | -0.04 |\n",
       "| 4 | 5 | right |  1.00 |\n",
       "\n"
      ],
      "text/plain": [
       "  step state action reward\n",
       "1 0    1     NA      0.00 \n",
       "2 1    2     right  -0.04 \n",
       "3 2    3     right  -0.04 \n",
       "4 3    4     right  -0.04 \n",
       "5 4    5     right   1.00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state <- 1\n",
    "trajectory <- data.frame(step = 0, state = state, action = NA, reward = 0)\n",
    "\n",
    "for (t in 1:10) {\n",
    "  action <- actions[which.max(Q[state, ])]\n",
    "  out <- step_env(state, action)\n",
    "\n",
    "  trajectory <- rbind(\n",
    "    trajectory,\n",
    "    data.frame(step = t, state = out$next_state, action = action, reward = out$reward)\n",
    "  )\n",
    "\n",
    "  state <- out$next_state\n",
    "  if (out$done) break\n",
    "}\n",
    "\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned policy should mostly choose `right` in states `1` to `4`, moving quickly to the goal state `5`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
