{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 5: Simulation and Bootstrap\n",
    "\n",
    "#### Zhentao Shi\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"graph/macau.jpg\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulation\n",
    "\n",
    "\n",
    "* check finite-sample performance\n",
    "* bootstrap, a data-driven inference procedure\n",
    "* generate non-standard distributions\n",
    "* approximate integrals with no analytic expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appetizer\n",
    "\n",
    "calculating $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.add_patch(plt.Rectangle((-1,-1), 2, 2, fill=False))\n",
    "ax.add_artist(plt.Circle((0, 0), 1, fill=False))\n",
    "x = np.random.uniform(-1, 1, size=100)\n",
    "y = np.random.uniform(-1, 1, size=100)\n",
    "ax.scatter(x, y, s=25, c='blue', marker='o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By laws of large numbers, $\\pi$ can be approximated by a stochastic algorithm\n",
    "\n",
    "$$\n",
    "E\\left[\\boldsymbol{1}\\left\\{ x^{2}+y^{2}\\leq1\\right\\} \\right] = \\frac{\\pi r^{2}}{\\left(2r\\right)^{2}}= \\frac{\\pi}{4}\n",
    "$$\n",
    "\n",
    "it implies  $\\pi=4\\times E[ 1 \\{  x^{2}+y^{2}\\leq1 \\}]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 20000000\n",
    "Z = 2 * np.random.rand(n, 2) - 1 # uniform distribution in [-1, 1]\n",
    "\n",
    "inside = np.mean(np.sqrt(np.sum(Z**2, axis=1)) <= 1)\n",
    "print(f\"The estimated pi = {inside * 4:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sample size can be made as large as the computer's memory permits.\n",
    "* Iterate it with average of averages and so on, for higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Writing Script\n",
    "\n",
    "* A script is a piece of code for a particular purpose. \n",
    "* Thousands of lines are not written from the beginning to the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Recursive development\n",
    "  * small manageable tasks\n",
    "  * test code constantly \n",
    "  * encapsulate into DIY functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Integrate small pieces into the super structure. \n",
    "* Add comments to the script to facilitate readability\n",
    "* Scprit (XXX.py) is more suitable than Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finite Sample Evaluation\n",
    "\n",
    "\n",
    "* Real world sample is finite\n",
    "* Asymptotic theory is a mathematical apparatus to approximate finite sample distributions\n",
    "* Modern econometric theory is built on asymptotics\n",
    "* Simulation is one way to evaluate the accuracy of approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "For the OLS estimator, \n",
    "\n",
    "* Classical views $X$ as fixed regressions and only cares about the randomness of the error term.\n",
    "* Modern econometrics textbook emphasizes that a random $X$ is more appropriate\n",
    "for econometrics applications. \n",
    "* In rigorous textbooks, the moment of $X$ is explicitly stated as $E[X_i X_i'] < \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* A Pareto distribution with shape coefficient between 1 and 2 has finite population mean, but infinite variance. \n",
    "* If $X$ follows a\n",
    "[Pareto distribution](https://en.wikipedia.org/wiki/Pareto_distribution) with shape coefficient 1.5, it violates the assumptions for OLS stated in most of econometric textbooks.\n",
    "* Question: Is asymptotic inferential theory for the OLS estimator valid? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We write a script to investigate this problem. The following steps develop the code.\n",
    "\n",
    " 1. given a sample size, get the OLS `b_hat` and its associated `t_value`.\n",
    " 2. wrap `t_value` as a user-defined function so that we can reuse it for many times.\n",
    " 3. given a sample size, report the size under two distributions.\n",
    " 4. wrap step 3 again as a user-defined function, ready for different sample sizes.\n",
    " 5. develop the super structure to connect the workhorse functions.\n",
    " 6. add comments and documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $t$-statistic\n",
    "\n",
    "$$\n",
    "\\sqrt{n} (\\hat{\\beta} - \\beta_0) |X = (X'X/n)^{-1}  (X' e /\\sqrt{n}),\n",
    "$$\n",
    "\n",
    "the $k$-th element of the vector coefficient conditional on $X$ is\n",
    "$$\n",
    "\\widehat{\\beta}_{k}|X=\\eta_{k}'\\widehat{\\beta}|X\n",
    "\\sim N\\left(\\beta_{k},\\sigma^{2}\\left(X'X\\right)_{kk}^{-1}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "def simulation(n, dist='normal', df=5):\n",
    "    \"\"\"\n",
    "    Simulate the t-value of the second parameter in a linear regression model\n",
    "    with n samples and the specified distribution of errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    if dist == 'normal':\n",
    "        e = np.random.normal(size=n)\n",
    "    elif dist == 't':\n",
    "        e = np.random.standard_t(df, size=n)\n",
    "\n",
    "    b0 = np.array([1, 2])\n",
    "    X = np.column_stack((np.ones(n), np.random.pareto(1.5, size=n)))\n",
    "    # the above X is generated from a Pareto distribution\n",
    "    Y = X.dot(b0) + e\n",
    "\n",
    "    bhat = np.linalg.inv(X.T @ X) @ (X.T @ Y)\n",
    "    bhat2 = bhat[1] # parameter we want to test\n",
    "\n",
    "    e_hat = Y - X.dot(bhat)\n",
    "    sigma_hat_square = np.sum(e_hat**2) / (n - 2)\n",
    "    sig_B = np.linalg.inv(X.T.dot(X)) * sigma_hat_square\n",
    "    t_value_2 = (bhat2 - b0[1]) / np.sqrt(sig_B[1, 1])\n",
    "\n",
    "    return t_value_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import t, norm\n",
    "\n",
    "def report(n, Rep):\n",
    "    # collect the test size from the two distributions\n",
    "    # this function contains some repetitive code, but is OK for such a simple one\n",
    "    TEST_SIZE = np.zeros(3)\n",
    "\n",
    "    # e ~ normal distribution, under which the t-dist is exact\n",
    "    # generate the t-statistics from the normal distribution\n",
    "    Res = np.array([simulation(n, 'normal') for i in range(Rep)])\n",
    "    \n",
    "    TEST_SIZE[0] = np.mean(np.abs(Res) > t.ppf(0.975, n-2))\n",
    "    TEST_SIZE[1] = np.mean(np.abs(Res) > norm.ppf(0.975))\n",
    "\n",
    "    # e ~ t-distribution, under which the exact distribution is complicated.\n",
    "    # we rely on asymptotic normal distribution for inference instead\n",
    "    Res = np.array([simulation(n, 't', df) for i in range(Rep)])\n",
    "    TEST_SIZE[2] = np.mean(np.abs(Res) > norm.ppf(0.975))\n",
    "\n",
    "    return TEST_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Rep = 1000\n",
    "df = 1  # t dist. with df = 1 is Cauchy\n",
    "\n",
    "# run the calculation of the empirical sizes for different sample sizes\n",
    "NN = [5, 10, 200, 2000, 20000]\n",
    "\n",
    "RES = pd.DataFrame(np.zeros((len(NN), 4)), columns=['n', 'exact', 'normal.asym', 'cauchy.asym'])\n",
    "\n",
    "for i, n in enumerate(NN): # loop over different sample sizes\n",
    "    # enumerate is a built-in Python function that returns an iterator containing the index and value of a list\n",
    "\n",
    "    RES.iloc[i, 0] = n\n",
    "    RES.iloc[i, 1:] = report(n, Rep)\n",
    "\n",
    "print(RES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simulation Results\n",
    "\n",
    "* 1st column: the error is normal, use exact distribution theory to find the critical value (according to $t$-distribution.)\n",
    "\n",
    "* 2nd column: still uses the normal error\n",
    "  * change the critical value to asymptotic normal distribution\n",
    "  \n",
    "* 3rd column: error distribution is Cauchy\n",
    "  * asymptotic approximation breaks down\n",
    "  * test size does not converge to the nominal 5%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observations\n",
    "\n",
    "* $X$ is always Pareto \n",
    "* distribution of $X$ doesn't matter in our simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Justification\n",
    "\n",
    "* Self-normalized $t$ statistic does not break down despite that $X'X/n$ does not converge\n",
    "* Regardless the distribution of $X$, when the error term is normal \n",
    "  * numerator follows $N(0,1)$\n",
    "  * demonimator follows $\\chi^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## World View about Uncertainty\n",
    "\n",
    "\n",
    "* Fundamental question: how to quantify uncertainty.\n",
    "  * data $(X_1,X_2,\\ldots,X_n)$\n",
    "  * sample mean $\\bar{X}$\n",
    "  * sample variance $s$\n",
    "  * frequentist confidence interval about the population mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Asymptotics is imaginary.\n",
    "* Let's be realistic: we have a finite sample $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrap\n",
    "\n",
    "* Let $X_1, X_2, \\ldots, X_n \\sim F$ be an i.i.d. sample of $n$ observations following a distribution $F$. \n",
    "* The finite sample distribution of a statistic $T_n(\\theta)\\sim G_n(\\cdot, F)$ usually depends on the sample size $n$, as well as the unknown true distribution $F$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Idea\n",
    "\n",
    "* Bootstrap replaces the unknown distribution $F$ in $G_n(\\cdot, F)$ by the empirical distribution function\n",
    "\n",
    "$$\n",
    "\\hat{F}_n(\\cdot) = n^{-1} \\sum_{i=1}^n 1\\{\\cdot \\leq X_i\\}\n",
    "$$\n",
    "\n",
    "* Bootstrap inference is drawn from the bootstrap distribution\n",
    "\n",
    "$$\n",
    "G_n(\\cdot, \\hat{F}_n)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Compare to Asymptotic Theory\n",
    "\n",
    "* Bootstrap is a finite-sample practice \n",
    "* It doesn't refer to an imaginary world where $n\\to \\infty$ at its face value\n",
    "\n",
    "\n",
    "* Asymptotic theory approximates $G_n(\\cdot, F)$ by its limit \n",
    "\n",
    "$$G(\\cdot, F) := \\lim_{n\\to\\infty} G_n(\\cdot, F).$$ \n",
    "\n",
    "* In many cases $G(\\cdot, F)$ is independent of $F$ and it becomes $G(\\cdot)$. Such a $T_n(\\theta)$ is called *asymptotically pivotal*, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "x = np.random.uniform(size=10)\n",
    "ecdf = ECDF(x)\n",
    "\n",
    "# get unique sorted values of x and corresponding values of y\n",
    "ux, uy = np.unique(ecdf.x, return_index=False, return_inverse=False, return_counts=False), ecdf.y[np.unique(ecdf.y, return_index=True, return_inverse=False, return_counts=False)[1]]\n",
    "\n",
    "# add last point (1,1) to the step function\n",
    "ux = np.concatenate((ux, [1]))\n",
    "uy = np.concatenate((uy, [1]))\n",
    "\n",
    "# create step function with horizontal lines\n",
    "plt.plot(ux[:-1], uy[:-1], drawstyle='steps-pre')\n",
    "plt.plot([ux[-2], ux[-1]], [uy[-2], uy[-2]], color='C0', linestyle='--')\n",
    "plt.plot([ux[-1], ux[-1]], [uy[-2], uy[-1]], color='C0', linestyle='--')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('ECDF for uniform distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nonparametric Bootstrap\n",
    "\n",
    "* Implementation of bootstrap is a simulation exercise. \n",
    "* In an i.i.d. environment, $n$ observations are drawn with equal weight and **with replacement** from the realized sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variants of Bootstrap Schemes\n",
    "\n",
    "* Block bootstrap: preserve dependence structure\n",
    "  * dependent dataset such as time series\n",
    "  * clustering data or networks\n",
    "\n",
    "* parametric bootstrap\n",
    "  * In regressions we fix the regressors $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "n = 9  # real sample size\n",
    "d0 = pd.DataFrame({'x': np.random.normal(size=n)})\n",
    "print(d0)\n",
    "\n",
    "# bootstrap\n",
    "boot_Rep = 3  # bootstrap 3 times\n",
    "d_boot = np.zeros((n, boot_Rep))  # save the bootstrap sample\n",
    "d_boot_index = np.zeros((n, boot_Rep), dtype=int)  # save the bootstrap sample index\n",
    "for b in range(boot_Rep):\n",
    "    boot_index = np.random.choice(d0.index, n, replace=True) # sampling with replacement\n",
    "    d_boot[:, b] = d0['x'].iloc[boot_index]\n",
    "    d_boot_index[:, b] = boot_index\n",
    "\n",
    "d_boot_df = pd.DataFrame(d_boot)  # convert the bootstrap samples to DataFrame\n",
    "print(d_boot_df)\n",
    "print(d_boot_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example**: In one sample, there is only one sample mean. How to compute the variance of the sample mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.var(d0['x'])/n # the variance of the sample mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if you forget the formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bootstrap Estimation\n",
    "\n",
    "* Bootstrap is nothing but a loop.\n",
    "* Bootstrap is convenient. \n",
    "  * Analytic formula of the variance of an econometric estimator can be complex to derive or code up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "boot_Rep = 1000  # bootstrap many times times\n",
    "d_boot = np.zeros((n, boot_Rep))  # save the bootstrap sample \n",
    "for b in range(boot_Rep):\n",
    "    boot_index = np.random.choice(n, n, replace=True)\n",
    "    d_boot[:, b] = d0['x'][boot_index]\n",
    " \n",
    "np.mean(d_boot, axis=0).var()  # standard deviation of the bootstrap means across boot samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bootstrap Test\n",
    "\n",
    "* Bootstrap is particularly helpful in inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* Test a hypothesis about the population mean. \n",
    "\n",
    "* Use $t$-statistic\n",
    "* Distribution of the sample is either\n",
    "  * normal\n",
    "  * zero-centered chi-square \n",
    "\n",
    "* We will show that the bootstrap test size is\n",
    "more precise than that of the asymptotic approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def T_stat(Y, mu):\n",
    "    n = len(Y)\n",
    "    return np.sqrt(n) * (np.mean(Y) - mu) / np.std(Y, ddof=1)\n",
    "\n",
    "def boot_test(Y, alpha, boot_Rep):\n",
    "    n = len(Y)\n",
    "    boot_T = []\n",
    "\n",
    "    for r in range(boot_Rep):\n",
    "        \n",
    "        resampled_Y = Y[np.random.choice(n, n, replace=True)]\n",
    "        boot_T.append(abs(T_stat(resampled_Y, np.mean(Y))))\n",
    "        # this is a key line\n",
    "        # `mu` must be replaced as mean(Y)\n",
    "\n",
    "    boot_critical_value = np.quantile(boot_T, 1 - alpha)\n",
    "    return boot_critical_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bootstrap World\n",
    "\n",
    "* The null hypothesis must be imposed no matter the hypothesized parameter is true value or not.\n",
    "\n",
    "* Bootstrap $t$-statistic is\n",
    "\n",
    "$$\n",
    "T^{*}_{n} = \\frac{\\sqrt{n} (\\bar{X^{*}} - \\bar{X}) } { s^{*}  }.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Design the Statistic\n",
    "\n",
    "\n",
    "* In the bootstrap world the **true** distribution is $F_n$\n",
    "* The bootstrap $t$-statistic is centered around $\\bar{X}$, the sample mean of $F_n$\n",
    "* If we wrongly center it around the population mean $\\theta$, then the test will have no power when the null hypothesis is false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following chuck of code report the rejection probability from three decision rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t, norm, chi2\n",
    "\n",
    "def compare():\n",
    "    if distribution == \"normal\":\n",
    "        X = np.random.normal(size=n)\n",
    "    elif distribution == \"chisq\":\n",
    "        X = chi2.rvs(n-1, size=n) - (n - 1)\n",
    "\n",
    "    t_value_X = T_stat(X, mu)\n",
    "\n",
    "    exact = abs(t_value_X) > t.ppf(1 - alpha/2, n-1)\n",
    "    asym = abs(t_value_X) > norm.ppf(1 - alpha/2)\n",
    "    boot_rule = abs(t_value_X) > boot_test(X, alpha, boot_Rep)\n",
    "\n",
    "    return np.array([exact, asym, boot_rule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "distribution = \"normal\"\n",
    "boot_Rep = 199\n",
    "MC_rep = 2000 # number of Monte Carlo replications\n",
    "alpha = 0.05\n",
    "mu = 0\n",
    "\n",
    "res = pd.DataFrame(index = range(MC_rep), columns=['exact', 'asym', 'boot'])\n",
    "\n",
    "for i in range(MC_rep):\n",
    "    res.iloc[i,:] = compare()\n",
    "\n",
    "res.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Results\n",
    "\n",
    "* The program reports the empirical size.\n",
    "* Nominal size of the test is 5%. \n",
    "* Bootstrap test is more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* When the underlying distribution is a $\\chi^2$, the exact distribution is difficult \n",
    "to derive analytically. \n",
    "* However, we can still compare the asymptotic size with the bootstrap size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "distribution = \"chisq\"\n",
    "boot_Rep = 199\n",
    "MC_rep = 2000\n",
    "alpha = 0.05\n",
    "mu = 0\n",
    "\n",
    "for i in range(MC_rep):\n",
    "    res.iloc[i,:] = compare()\n",
    "\n",
    "res.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Here the \"exact test\" is no longer exact. \n",
    "* The asymptotic test works fairly reasonable\n",
    "* Bootstrap is closer to the nominal size 5%."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
