{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 5: Simulations and Uncertainty\n",
    "\n",
    "#### Zhentao Shi\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"graph/macau.jpg\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulation\n",
    "\n",
    "\n",
    "* check finite-sample performance\n",
    "* bootstrap, a data-driven inference procedure\n",
    "* generate non-standard distributions\n",
    "* approximate integrals with no analytic expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Appetizer\n",
    "\n",
    "calculating $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.add_patch(plt.Rectangle((-1,-1), 2, 2, fill=False))\n",
    "ax.add_artist(plt.Circle((0, 0), 1, fill=False))\n",
    "x = np.random.uniform(-1, 1, size=100)\n",
    "y = np.random.uniform(-1, 1, size=100)\n",
    "ax.scatter(x, y, s=25, c='blue', marker='o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By laws of large numbers, $\\pi$ can be approximated by a stochastic algorithm\n",
    "\n",
    "$$\n",
    "E\\left[\\boldsymbol{1}\\left\\{ x^{2}+y^{2}\\leq1\\right\\} \\right] = \\frac{\\pi r^{2}}{\\left(2r\\right)^{2}}= \\frac{\\pi}{4}\n",
    "$$\n",
    "\n",
    "it implies  $\\pi=4\\times E[ 1 \\{  x^{2}+y^{2}\\leq1 \\}]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 20000000\n",
    "\n",
    "# set a random seed\n",
    "np.random.seed(20250221)\n",
    "\n",
    "Z = 2 * np.random.rand(n, 2) - 1 # uniform distribution in [-1, 1]\n",
    "\n",
    "inside = np.mean(np.sqrt(np.sum(Z**2, axis=1)) <= 1)\n",
    "print(f\"The estimated pi = {inside * 4:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sample size can be made as large as the computer's memory permits.\n",
    "* Iterate it with average of averages and so on, for higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handwriting Scripts\n",
    "\n",
    "This course promotes vibe coding.\n",
    "\n",
    "Human wrote code by hand, before AI.\n",
    "\n",
    "* A script is a piece of code for a particular purpose. \n",
    "* Thousands of lines are not written from the beginning to the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Recursive development\n",
    "  * small manageable tasks\n",
    "  * test code constantly \n",
    "  * encapsulate into DIY functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Integrate small pieces into the super structure. \n",
    "* Add comments to the script to facilitate readability\n",
    "* Scprit (XXX.py) is more suitable than Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finite Sample Evaluation\n",
    "\n",
    "\n",
    "* Real world sample is finite\n",
    "* Asymptotic theory is a mathematical apparatus to approximate finite sample distributions\n",
    "* Modern econometric theory is built on asymptotics\n",
    "* Simulation is one way to evaluate the accuracy of approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "For the OLS estimator, \n",
    "\n",
    "* Classical views $X$ as fixed regressions and only cares about the randomness of the error term.\n",
    "* Modern econometrics textbook emphasizes that a random $X$ is more appropriate\n",
    "for econometrics applications. \n",
    "* In rigorous textbooks, the moment of $X$ is explicitly stated as $E[X_i X_i'] < \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* A Pareto distribution with shape coefficient between 1 and 2 has finite population mean, but infinite variance. \n",
    "* If $X$ follows a\n",
    "[Pareto distribution](https://en.wikipedia.org/wiki/Pareto_distribution) with shape coefficient 1.5, it violates the assumptions for OLS stated in most of econometric textbooks.\n",
    "* Question: Is asymptotic inferential theory for the OLS estimator valid? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We write a script to investigate this problem. The following steps develop the code.\n",
    "\n",
    " 1. given a sample size, get the OLS `b_hat` and its associated `t_value`.\n",
    " 2. wrap `t_value` as a user-defined function so that we can reuse it for many times.\n",
    " 3. given a sample size, report the size under two distributions.\n",
    " 4. wrap step 3 again as a user-defined function, ready for different sample sizes.\n",
    " 5. develop the super structure to connect the workhorse functions.\n",
    " 6. add comments and documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $t$-statistic\n",
    "\n",
    "$$\n",
    "\\sqrt{n} (\\hat{\\beta} - \\beta_0) |X = (X'X/n)^{-1}  (X' e /\\sqrt{n}),\n",
    "$$\n",
    "\n",
    "the $k$-th element of the vector coefficient conditional on $X$ is\n",
    "$$\n",
    "\\widehat{\\beta}_{k}|X=\\eta_{k}'\\widehat{\\beta}|X\n",
    "\\sim N\\left(\\beta_{k},\\sigma^{2}\\left(X'X\\right)_{kk}^{-1}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "def simulation(n, dist='normal', df=5):\n",
    "    \"\"\"\n",
    "    Simulate the t-value of the second parameter in a linear regression model\n",
    "    with n samples and the specified distribution of errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    if dist == 'normal':\n",
    "        e = np.random.normal(size=n)\n",
    "    elif dist == 't':\n",
    "        e = np.random.standard_t(df, size=n)\n",
    "\n",
    "    b0 = np.array([1, 2])\n",
    "    X = np.column_stack((np.ones(n), np.random.pareto(1.5, size=n)))\n",
    "    # the above X is generated from a Pareto distribution\n",
    "    Y = X.dot(b0) + e\n",
    "\n",
    "    bhat = np.linalg.inv(X.T @ X) @ (X.T @ Y)\n",
    "    bhat2 = bhat[1] # parameter we want to test\n",
    "\n",
    "    e_hat = Y - X.dot(bhat)\n",
    "    sigma_hat_square = np.sum(e_hat**2) / (n - 2)\n",
    "    sig_B = np.linalg.inv(X.T.dot(X)) * sigma_hat_square\n",
    "    t_value_2 = (bhat2 - b0[1]) / np.sqrt(sig_B[1, 1])\n",
    "\n",
    "    return t_value_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import t, norm\n",
    "\n",
    "def report(n, Rep):\n",
    "    # collect the test size from the two distributions\n",
    "    # this function contains some repetitive code, but is OK for such a simple one\n",
    "    TEST_SIZE = np.zeros(3)\n",
    "\n",
    "    # e ~ normal distribution, under which the t-dist is exact\n",
    "    # generate the t-statistics from the normal distribution\n",
    "    Res = np.array([simulation(n, 'normal') for i in range(Rep)])\n",
    "    \n",
    "    TEST_SIZE[0] = np.mean(np.abs(Res) > t.ppf(0.975, n-2))\n",
    "    TEST_SIZE[1] = np.mean(np.abs(Res) > norm.ppf(0.975))\n",
    "\n",
    "    # e ~ t-distribution, under which the exact distribution is complicated.\n",
    "    # we rely on asymptotic normal distribution for inference instead\n",
    "    Res = np.array([simulation(n, 't', df) for i in range(Rep)])\n",
    "    TEST_SIZE[2] = np.mean(np.abs(Res) > norm.ppf(0.975))\n",
    "\n",
    "    return TEST_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Rep = 1000\n",
    "df = 1  # t dist. with df = 1 is Cauchy\n",
    "\n",
    "# run the calculation of the empirical sizes for different sample sizes\n",
    "NN = [5, 10, 200, 2000, 20000]\n",
    "\n",
    "RES = pd.DataFrame(np.zeros((len(NN), 4)), columns=['n', 'exact', 'normal.asym', 'cauchy.asym'])\n",
    "\n",
    "for i, n in enumerate(NN): # loop over different sample sizes\n",
    "    # enumerate is a built-in Python function that returns an iterator containing the index and value of a list\n",
    "\n",
    "    RES.iloc[i, 0] = n\n",
    "    RES.iloc[i, 1:] = report(n, Rep)\n",
    "\n",
    "print(RES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simulation Results\n",
    "\n",
    "* 1st column: the error is normal, use exact distribution theory to find the critical value (according to $t$-distribution.)\n",
    "\n",
    "* 2nd column: still uses the normal error\n",
    "  * change the critical value to asymptotic normal distribution\n",
    "  \n",
    "* 3rd column: error distribution is Cauchy\n",
    "  * asymptotic approximation breaks down\n",
    "  * test size does not converge to the nominal 5%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Observations\n",
    "\n",
    "* $X$ is always Pareto \n",
    "* distribution of $X$ doesn't matter in our simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Justification\n",
    "\n",
    "* Self-normalized $t$ statistic does not break down despite that $X'X/n$ does not converge\n",
    "* Regardless the distribution of $X$, when the error term is normal \n",
    "  * numerator follows $N(0,1)$\n",
    "  * demonimator follows $\\chi^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Agent\n",
    "\n",
    "to be implemented in SCRP.\n",
    "\n",
    "* **Prompt**: Create a demo of using simulations to evaluate the value-at-risk of a portfolio of 5 assets. Try normal distributions and heavy-tail distributions. Maintain the correlation between the assets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18dec37",
   "metadata": {},
   "source": [
    "## Application of Simulation: Portfolio VaR \n",
    "\n",
    "(from vibe coding)\n",
    "\n",
    "We simulate a portfolio of 5 assets and compare Value-at-Risk (VaR) under:\n",
    "\n",
    "1. A multivariate normal return model.\n",
    "2. A multivariate Student-$t$ model (heavy tails).\n",
    "\n",
    "Both models use the same target correlation matrix so cross-asset dependence is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng(5821)\n",
    "\n",
    "# Portfolio setup\n",
    "n_assets = 5\n",
    "n_sims = 200000\n",
    "weights = np.array([0.25, 0.20, 0.20, 0.20, 0.15])\n",
    "\n",
    "# Daily expected returns (example values)\n",
    "mu = np.array([0.0005, 0.0004, 0.00045, 0.00035, 0.0003])\n",
    "\n",
    "# Daily volatilities\n",
    "vol = np.array([0.012, 0.010, 0.011, 0.009, 0.013])\n",
    "\n",
    "# Correlation matrix (must be symmetric positive definite)\n",
    "corr = np.array([\n",
    "    [1.00, 0.55, 0.40, 0.30, 0.35],\n",
    "    [0.55, 1.00, 0.45, 0.25, 0.30],\n",
    "    [0.40, 0.45, 1.00, 0.50, 0.20],\n",
    "    [0.30, 0.25, 0.50, 1.00, 0.15],\n",
    "    [0.35, 0.30, 0.20, 0.15, 1.00],\n",
    "])\n",
    "\n",
    "# Covariance used by the Gaussian model\n",
    "cov = np.outer(vol, vol) * corr\n",
    "\n",
    "# Utility: VaR and ES from simulated portfolio returns\n",
    "def var_es(returns, alpha=0.99):\n",
    "    # Loss = -return. VaR_{alpha} is alpha-quantile of loss.\n",
    "    losses = -returns\n",
    "    var = np.quantile(losses, alpha)\n",
    "    es = losses[losses >= var].mean()\n",
    "    return var, es\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e24ad07",
   "metadata": {},
   "source": [
    "### 1) Correlated Normal Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate normal asset returns\n",
    "r_norm = rng.multivariate_normal(mean=mu, cov=cov, size=n_sims)\n",
    "port_r_norm = r_norm @ weights\n",
    "\n",
    "# Check that simulated cross-asset correlation matches target\n",
    "corr_norm_sim = np.corrcoef(r_norm, rowvar=False)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"Target correlation matrix:\", corr)\n",
    "print(\"Simulated correlation (normal):\", corr_norm_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a83b35",
   "metadata": {},
   "source": [
    "### 2) Correlated Heavy-Tail Simulation via Left-Tail-Dependent Copula\n",
    "\n",
    "We use a **Clayton copula** to create strong lower-tail dependence:\n",
    "\n",
    "- Clayton parameter `theta > 0` controls lower-tail dependence.\n",
    "- Pairwise lower-tail dependence is\n",
    "\n",
    "$$\n",
    "\\lambda_L = 2^{-1/\t\\theta}.\n",
    "$$\n",
    "\n",
    "Then we map copula uniforms to Student-`t` marginals with `t.ppf`.\n",
    "\n",
    "Note: This simple Clayton construction is exchangeable (same dependence strength across pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "nu = 3\n",
    "# Clayton copula parameter: positive => lower-tail dependence.\n",
    "# theta=0.6 gives noticeable left-tail dependence while keeping average\n",
    "# linear correlation closer to the target matrix than very large theta.\n",
    "theta = 0.8\n",
    "lambda_L = 2 ** (-1 / theta)\n",
    "print(f\"Theoretical lower-tail dependence lambda_L = {lambda_L:.3f}\")\n",
    "\n",
    "# Sample Clayton copula uniforms (Marshall-Olkin representation)\n",
    "w = rng.gamma(shape=1 / theta, scale=1.0, size=n_sims)\n",
    "e = rng.exponential(scale=1.0, size=(n_sims, n_assets))\n",
    "u_cop = (1 + e / w[:, None]) ** (-1 / theta)\n",
    "\n",
    "# Avoid exact 0/1 before inverse CDF transform\n",
    "eps = 1e-12\n",
    "u_cop = np.clip(u_cop, eps, 1 - eps)\n",
    "\n",
    "# Heavy-tail marginals via inverse Student-t CDF\n",
    "x_t = t.ppf(u_cop, df=nu)\n",
    "\n",
    "# Standardize to unit variance (nu > 2), then scale to target vols\n",
    "x_t = x_t * np.sqrt((nu - 2) / nu)\n",
    "r_t = mu + x_t * vol\n",
    "port_r_t = r_t @ weights\n",
    "\n",
    "corr_t_sim = np.corrcoef(r_t, rowvar=False)\n",
    "spear_t_sim = pd.DataFrame(r_t).corr(method=\"spearman\").to_numpy()\n",
    "\n",
    "# Compare average off-diagonal correlation to target matrix\n",
    "def avg_offdiag(a):\n",
    "    n = a.shape[0]\n",
    "    return (a.sum() - np.trace(a)) / (n * (n - 1))\n",
    "\n",
    "avg_target_corr = avg_offdiag(corr)\n",
    "avg_sim_corr = avg_offdiag(corr_t_sim)\n",
    "\n",
    "# Empirical lower-tail dependence for one pair (assets 1 and 2)\n",
    "q = 0.05\n",
    "lambda_L_hat = ((u_cop[:, 0] <= q) & (u_cop[:, 1] <= q)).mean() / q\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(\"Target correlation matrix:\", corr)\n",
    "print(\"Simulated Pearson correlation (Clayton-copula t):\", corr_t_sim)\n",
    "print(\"Simulated Spearman correlation (Clayton-copula t):\", spear_t_sim)\n",
    "print(f\"Average off-diagonal corr: target={avg_target_corr:.3f}, simulated={avg_sim_corr:.3f}\")\n",
    "print(f\"Empirical lower-tail dependence (q={q:.2f}, assets 1&2): {lambda_L_hat:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b6b76",
   "metadata": {},
   "source": [
    "### 3) VaR / ES Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc97c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.95, 0.99]\n",
    "rows = []\n",
    "for a in alphas:\n",
    "    var_n, es_n = var_es(port_r_norm, alpha=a)\n",
    "    var_t, es_t = var_es(port_r_t, alpha=a)\n",
    "    rows.append([f\"{int(a*100)}%\", \"Normal\", var_n, es_n])\n",
    "    rows.append([f\"{int(a*100)}%\", f\"Student-t (nu={nu})\", var_t, es_t])\n",
    "\n",
    "res = pd.DataFrame(rows, columns=[\"Confidence\", \"Model\", \"VaR\", \"ES\"])\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of portfolio return tails\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(port_r_norm, bins=200, density=True, alpha=0.55, label=\"Normal\")\n",
    "plt.hist(port_r_t, bins=200, density=True, alpha=0.45, label=f\"Student-t (nu={nu})\")\n",
    "plt.xlim(-0.08, 0.04)\n",
    "plt.title(\"Portfolio Return Distribution: Normal vs Heavy-Tail\")\n",
    "plt.xlabel(\"Daily portfolio return\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5071e",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- The Clayton-copula model injects strong **left-tail dependence** (joint crashes are more likely).\n",
    "- Combined with Student-`t` marginals, this typically raises 99% VaR and ES versus the normal model.\n",
    "- This setup is useful when downside co-movement risk is a primary concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World View about Uncertainty\n",
    "\n",
    "\n",
    "* Fundamental question: how to quantify uncertainty.\n",
    "  * data $(X_1,X_2,\\ldots,X_n)$\n",
    "  * sample mean $\\bar{X}$\n",
    "  * sample variance $s$\n",
    "  * frequentist confidence interval about the population mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Asymptotics is imaginary.\n",
    "* Let's be realistic: we have a finite sample $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap\n",
    "\n",
    "* Let $X_1, X_2, \\ldots, X_n \\sim F$ be an i.i.d. sample of $n$ observations following a distribution $F$. \n",
    "* The finite sample distribution of a statistic $T_n(\\theta)\\sim G_n(\\cdot, F)$ usually depends on the sample size $n$, as well as the unknown true distribution $F$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Idea\n",
    "\n",
    "* Bootstrap replaces the unknown distribution $F$ in $G_n(\\cdot, F)$ by the empirical distribution function\n",
    "\n",
    "$$\n",
    "\\hat{F}_n(\\cdot) = n^{-1} \\sum_{i=1}^n 1\\{\\cdot \\leq X_i\\}\n",
    "$$\n",
    "\n",
    "* Bootstrap inference is drawn from the bootstrap distribution\n",
    "\n",
    "$$\n",
    "G_n(\\cdot, \\hat{F}_n)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Asymptotic Theory\n",
    "\n",
    "* Bootstrap is a finite-sample practice \n",
    "* It doesn't refer to an imaginary world where $n\\to \\infty$ at its face value\n",
    "\n",
    "\n",
    "* Asymptotic theory approximates $G_n(\\cdot, F)$ by its limit \n",
    "\n",
    "$$\n",
    "G(\\cdot, F) := \\lim_{n\\to\\infty} G_n(\\cdot, F).\n",
    "$$ \n",
    "\n",
    "* In many cases $G(\\cdot, F)$ is independent of $F$ and it becomes $G(\\cdot)$. Such a $T_n(\\theta)$ is called *asymptotically pivotal*, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "x = np.random.beta(2,2,size=10)\n",
    "ecdf = ECDF(x)\n",
    "\n",
    "# get unique sorted values of x and corresponding values of y\n",
    "ux, uy = np.unique(ecdf.x, return_index=False, return_inverse=False, return_counts=False), ecdf.y[np.unique(ecdf.y, return_index=True, return_inverse=False, return_counts=False)[1]]\n",
    "\n",
    "# add last point (1,1) to the step function\n",
    "ux = np.concatenate((ux, [1]))\n",
    "uy = np.concatenate((uy, [1]))\n",
    "\n",
    "# create step function with horizontal lines\n",
    "plt.plot(ux[:-1], uy[:-1], drawstyle='steps-pre')\n",
    "plt.plot([ux[-2], ux[-1]], [uy[-2], uy[-2]], color='C0', linestyle='--')\n",
    "plt.plot([ux[-1], ux[-1]], [uy[-2], uy[-1]], color='C0', linestyle='--')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('ECDF for uniform distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonparametric Bootstrap\n",
    "\n",
    "* Implementation of bootstrap is a simulation exercise. \n",
    "* In an i.i.d. environment, $n$ observations are drawn with equal weight and **with replacement** from the realized sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants of Bootstrap Schemes\n",
    "\n",
    "* Block bootstrap: preserve dependence structure\n",
    "  * dependent dataset such as time series\n",
    "  * clustering data or networks\n",
    "\n",
    "* parametric bootstrap\n",
    "  * In regressions we fix the regressors $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "n = 9  # real sample size\n",
    "d0 = pd.DataFrame({'x': np.random.normal(size=n)})\n",
    "print(d0)\n",
    "\n",
    "# bootstrap\n",
    "boot_Rep = 3  # bootstrap 3 times\n",
    "d_boot = np.zeros((n, boot_Rep))  # save the bootstrap sample\n",
    "d_boot_index = np.zeros((n, boot_Rep), dtype=int)  # save the bootstrap sample index\n",
    "for b in range(boot_Rep):\n",
    "    boot_index = np.random.choice(d0.index, n, replace=True) # sampling with replacement\n",
    "    d_boot[:, b] = d0['x'].iloc[boot_index]\n",
    "    d_boot_index[:, b] = boot_index\n",
    "\n",
    "d_boot_df = pd.DataFrame(d_boot)  # convert the bootstrap samples to DataFrame\n",
    "print(d_boot_df)\n",
    "print(d_boot_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: In one sample, there is only one sample mean. How to compute the variance of the sample mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(d0['x'])/n # the variance of the sample mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you forget the formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Estimation\n",
    "\n",
    "* Bootstrap is simply a loop.\n",
    "* Bootstrap is convenient. \n",
    "  * Analytic formula of the variance of an econometric estimator can be complex to derive or code up.\n",
    "  * e.g.: MLE, in particular misspecified MLE (Hessian, Fisher...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_Rep = 10000  # bootstrap many times times\n",
    "d_boot = np.zeros((n, boot_Rep))  # save the bootstrap sample \n",
    "for b in range(boot_Rep):\n",
    "    boot_index = np.random.choice(n, n, replace=True)\n",
    "    d_boot[:, b] = d0['x'][boot_index]\n",
    " \n",
    "np.mean(d_boot, axis=0).var()  # standard deviation of the bootstrap means across boot samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Test\n",
    "\n",
    "* Bootstrap is particularly helpful in inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "* Test a hypothesis about the population mean. \n",
    "\n",
    "* Use $t$-statistic\n",
    "* Distribution of the sample is either\n",
    "  * normal\n",
    "  * zero-centered chi-square \n",
    "\n",
    "* We will show that the bootstrap test size is\n",
    "more precise than that of the asymptotic approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def T_stat(Y, mu):\n",
    "    n = len(Y)\n",
    "    return np.sqrt(n) * (np.mean(Y) - mu) / np.std(Y, ddof=1)\n",
    "\n",
    "def boot_test(Y, alpha, boot_Rep):\n",
    "    n = len(Y)\n",
    "    boot_T = []\n",
    "\n",
    "    for r in range(boot_Rep):\n",
    "        \n",
    "        resampled_Y = Y[np.random.choice(n, n, replace=True)]\n",
    "        boot_T.append(abs(T_stat(resampled_Y, np.mean(Y))))\n",
    "        # this is a key line\n",
    "        # `mu` must be replaced as mean(Y)\n",
    "\n",
    "    boot_critical_value = np.quantile(boot_T, 1 - alpha)\n",
    "    return boot_critical_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.random.normal(size = 20)\n",
    "T_stat(Y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_test(Y, 0.05, 1000) # bootstrap critical value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee21fc3c",
   "metadata": {},
   "source": [
    "## Bootstrap Application: Market Risk for an Equity Trading Desk\n",
    "\n",
    "A risk team often reports **1-day Value-at-Risk (VaR)** and **Expected Shortfall (ES)** to management.\n",
    "Because tail risk is hard to estimate analytically, bootstrap is a practical way to quantify uncertainty in these risk metrics.\n",
    "\n",
    "In this example, we create a stylized daily return series for a $50 million equity portfolio and use bootstrap to build confidence intervals for 99% VaR and ES.\n",
    "\n",
    "In this exercise, there is only one sample of n observations. There is no repeated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efdb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(5821)\n",
    "\n",
    "# Stylized daily returns: heavy tails + changing volatility (industry-like behavior)\n",
    "n_days = 252 * 3  # about 3 years of trading days\n",
    "portfolio_value = 50_000_000\n",
    "\n",
    "vol_regime = np.random.choice([0.008, 0.015, 0.025], size=n_days, p=[0.6, 0.3, 0.1])\n",
    "shock = np.random.standard_t(df=5, size=n_days) / np.sqrt(5 / (5 - 2))\n",
    "# when the true distribution is a \"t\", the tail prob is difficult to analyze \n",
    "\n",
    "daily_returns = 0.0002 + vol_regime * shock\n",
    "\n",
    "daily_pnl = portfolio_value * daily_returns\n",
    "\n",
    "print(f\"Sample size: {n_days} trading days\")\n",
    "print(f\"Average daily return: {daily_returns.mean():.4%}\")\n",
    "print(f\"Daily volatility: {daily_returns.std():.4%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point estimates from the observed sample\n",
    "alpha = 0.01  # 99% VaR\n",
    "var_99 = -np.quantile(daily_pnl, alpha)\n",
    "es_99 = -daily_pnl[daily_pnl <= np.quantile(daily_pnl, alpha)].mean()\n",
    "\n",
    "# Bootstrap uncertainty for VaR and ES\n",
    "B = 5000\n",
    "boot_var = np.empty(B)\n",
    "boot_es = np.empty(B)\n",
    "\n",
    "n = len(daily_pnl)\n",
    "for b in range(B):\n",
    "    sample = np.random.choice(daily_pnl, size=n, replace=True)\n",
    "    q = np.quantile(sample, alpha)\n",
    "    boot_var[b] = -q\n",
    "    boot_es[b] = -sample[sample <= q].mean()\n",
    "\n",
    "# 95% percentile bootstrap confidence intervals\n",
    "var_ci = np.quantile(boot_var, [0.025, 0.975])\n",
    "es_ci = np.quantile(boot_es, [0.025, 0.975])\n",
    "\n",
    "print(\"99% 1-day risk estimates (USD):\")\n",
    "print(f\"VaR point estimate: ${var_99:,.0f}\")\n",
    "print(f\"VaR 95% CI:       [${var_ci[0]:,.0f}, ${var_ci[1]:,.0f}]\")\n",
    "print(f\"ES point estimate: ${es_99:,.0f}\")\n",
    "print(f\"ES 95% CI:        [${es_ci[0]:,.0f}, ${es_ci[1]:,.0f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726344a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].hist(boot_var, bins=40, color=\"#1f77b4\", alpha=0.8)\n",
    "ax[0].axvline(var_99, color=\"black\", linestyle=\"--\", label=\"Point estimate\")\n",
    "ax[0].set_title(\"Bootstrap Distribution of 99% VaR\")\n",
    "ax[0].set_xlabel(\"VaR (USD)\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].hist(boot_es, bins=40, color=\"#d62728\", alpha=0.8)\n",
    "ax[1].axvline(es_99, color=\"black\", linestyle=\"--\", label=\"Point estimate\")\n",
    "ax[1].set_title(\"Bootstrap Distribution of 99% ES\")\n",
    "ax[1].set_xlabel(\"ES (USD)\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c1a94",
   "metadata": {},
   "source": [
    "### Interpretation for industry practice\n",
    "\n",
    "- VaR gives a threshold loss at a chosen confidence level (here, 99%).\n",
    "- ES captures the *average* loss in the worst 1% tail and is typically more tail-sensitive than VaR.\n",
    "- Bootstrap confidence intervals communicate estimation uncertainty to risk committees and can support capital or limit-setting discussions.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
