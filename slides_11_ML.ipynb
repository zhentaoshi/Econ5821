{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning \n",
    "\n",
    "### Zhentao Shi\n",
    "\n",
    "<img src=\"graph/Ada_Lovelace.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"graph/Lord_Byron.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To Lord Byron, from Ada\n",
    "\n",
    "(by MS Copilot)\n",
    "\n",
    "You were the bard of passion and of fire\n",
    "The one who sang of freedom and desire\n",
    "But you were also the source of my regret\n",
    "The one who left me with a heavy debt\n",
    "\n",
    "You never saw me as your daughter true\n",
    "You never cared for what I could pursue\n",
    "You never praised me for my skill and art\n",
    "You never valued my poetic heart\n",
    "\n",
    "You gave me only half your noble name\n",
    "And half your restless spirit that I claim\n",
    "But you also gave me half your curse\n",
    "And half your madness that I must disperse\n",
    "\n",
    "You were the scourge of tyrants and of kings\n",
    "The one who dared to fly with broken wings\n",
    "But you were also the slave of your own lust\n",
    "The one who broke the hearts of those who trust\n",
    "\n",
    "You lived a life of glory and of fame\n",
    "You died a martyr in a foreign game\n",
    "But you also left a trail of sin and woe\n",
    "And a riddle that I long to know\n",
    "\n",
    "You are the father I can never meet\n",
    "The one who haunts me in my every beat\n",
    "But you are also the muse I can't deny\n",
    "The one who spurs me to create and try\n",
    "\n",
    "You are the bard of passion and of fire\n",
    "And I am the bard of logic and of wire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Machine learning and artificial intelligence:\n",
    "\n",
    "* Technology or alchemy?\n",
    "* Statistics or biology?\n",
    "\n",
    "* [Tom Sargent](https://www.project-syndicate.org/commentary/artificial-intelligence-new-economic-models-by-thomas-j-sargent-2019-11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference\n",
    "\n",
    "* [ISLR] James, Gareth., Witten, Daniela., Hastie, Trevor., & Tibshirani, Robert. (2017). An introduction to statistical learning.  (Open access at https://www.statlearning.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Athey (2018) \n",
    "* Mullainathan and Spiess (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* Connection between $X$ and $Y$\n",
    "* Regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A set of data fitting procedures focusing on out-of-sample prediction\n",
    "* Repeat a scientific experiment for $n$ times and obtain a dataset $(y_i, x_i)_{i=1}^n$.\n",
    "* How to best predict $y_{n+1}$ given $x_{n+1}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "* Only about $X$\n",
    "* Density estimation, principal component analysis, and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Conventional Statistics\n",
    "\n",
    "* Consistency\n",
    "* Asymptotic distribution (hopefully normal)\n",
    "* Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Learning's Responses\n",
    "\n",
    "* Efficiency is mostly irrelevant given big data\n",
    "* Statistical inference may not be the goal\n",
    "    * Recommendation system on Amazon or Taobao\n",
    "    * Care about the prediction accuracy, not the causal link\n",
    "* Is there a data generating process (DGP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonparametric Estimation\n",
    "\n",
    "* *Parametric*: a finite number of parameters\n",
    "* *Nonparametric*: an infinite number of parameters\n",
    "\n",
    "* Some ideas in nonparametric estimation is directly related to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Density Estimation\n",
    "\n",
    "* Density estimation given a sample $(x_1,\\ldots,x_n)$\n",
    "* If drawn from a parametric family, MLE for estimation\n",
    "* Misspecification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Histogram is nonparametric\n",
    "    * If grid too fine, small bias but large variance\n",
    "    * If grid too coarse, small variance but large bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAADFBMVEUAAADT09P/AAD////u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\nAJEAEOAfdRqZVSGheqoAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n <- 200\n",
    "\n",
    "par(mfrow = c(3, 3))\n",
    "par(mar = c(1, 1, 1, 1))\n",
    "\n",
    "x_base <- seq(0.01,1,by = 0.01)\n",
    "breaks_list = c(4, 12, 60)\n",
    "\n",
    "for (ii in 1:3){\n",
    "  x <- rbeta(n, 2, 2) # beta distribution\n",
    "  for ( bb in breaks_list){\n",
    "    hist(x, breaks = bb, main=\"\", freq = FALSE, ylim = c(0,3),xlim = c(0,1))\n",
    "    lines( y = dbeta( x_base, 2, 2), x = x_base , col = \"red\" )\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variance-Bias Tradeoff\n",
    "\n",
    "![](graph/bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Conditional Mean\n",
    "\n",
    "* Conditional mean $$f(x) = E[y_i |x_i = x]$$ given a sample $(y_i, x_i)$. \n",
    "* Solve \n",
    "$$\n",
    "\\min_f E[ (y_i - f(x_i) )^2 ]\n",
    "$$\n",
    "* In general $f(x)$ is a nonlinear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Restrict the class of functions to search for minimizer\n",
    "    * Assume differentiability\n",
    "* One way is kernel method based on density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Series Estimation\n",
    "\n",
    "* Series expansion to approximate $f(x)$\n",
    "* Generates many additive regressors\n",
    "    * Ex: bounded, continuous and differentiate function has a series\n",
    "representation $f(x) = \\sum_{k=0}^{\\infty} \\beta_k \\cos (\\frac{k}{2}\\pi x )$.\n",
    "    * In finite sample, choose a finite $K$, usually much smaller than $n$\n",
    "    * Asymptotically $K \\to \\infty$ as $n \\to \\infty$ so that\n",
    "$$\n",
    "f_K(x) = \\sum_{k=0}^{K} \\beta_k \\cos \\left(\\frac{k}{2}\\pi x \\right) \\to f(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Bias-variance trade-off\n",
    "    * Big $K$: small bias and large variance \n",
    "    * Small $K$: small variance and large bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization\n",
    "\n",
    "* Specify a sufficiently large $K$, and then add a penalty term to control the complexity\n",
    "* Eg: *Ridge regression*: \n",
    "$$\n",
    "\\min_\\beta \\  \\frac{1}{2n}  \\sum_{i=1}^n \\left(y_i - \\sum_{k=0}^{K} \\beta_k f_k(x_i) \\right)^2\n",
    "+ \\lambda \\sum_{k=0}^K \\beta_k^2,\n",
    "$$\n",
    "where $\\lambda$ is the tuning parameter such that $\\lambda \\to 0$ as $n\\to \\infty$, and\n",
    "$f_k(x_i) = \\cos \\left(\\frac{k}{2}\\pi x_i \\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In compact notation, let $Y=(y_1,\\ldots,y_n)'$ and\n",
    "$X = (X_{ik} = f_k(x_i) )$, the above problem can be written as\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\Vert \\beta \\Vert_2 ^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tuning Parameter\n",
    "\n",
    "* *Information criterion*: AIC, BIC\n",
    "* *Cross validation*\n",
    "\n",
    "\n",
    "* Active statistical research, but has little economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Econometrics Workflow\n",
    "\n",
    "![](graph/metric_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "![ ](graph/ML_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "\n",
    "* Machine learning's main purpose is often prediction\n",
    "* Agnostic about the DGP.\n",
    "* Models are measured by their performance in prediction.\n",
    "* Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Training dataset\n",
    "* Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Testing sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Caret` Package\n",
    "\n",
    "* R package `caret` (Classification And REgression Training): a framework for many machine learning methods\n",
    "* The function [`createDataPartition`](https://topepo.github.io/caret/data-splitting.html)\n",
    "splits the sample for both cross sectional data and time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Validation (cross sectional data)\n",
    "\n",
    "* $S$-fold cross validation partitions the dataset into $S$ disjoint sections\n",
    "* Each iteration picks one of the sections as the (quasi) validation sample\n",
    "* The other $S-1$ sections as the training sample.\n",
    "* Compute an out-of-sample goodness-of-fit measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goodness of Fit (Out of Sample)\n",
    "\n",
    "* *Mean-squared prediction error* ${n_v}^{-1} \\sum_{i \\in val} (y_i - \\hat{y}_i)^2$ where $val$ is the validation set and $n_v$ is its cardinality, \n",
    "* *Mean-absolute prediction error* ${n_v}^{-1}\\sum_{i \\in val} |y_i - \\hat{y}_i|$. \n",
    "* *Out of sample R-squared* (OOS $R^2$):\n",
    "\n",
    "$$\n",
    "1 - \\frac{{n_v}^{-1} \\sum_{i \\in val} (y_i - \\hat{y}_i)^2}{{n_v}^{-1} \\sum_{i \\in val} y_i^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Repeat this process for $S$ times so that each of the $S$ sections are treated as the validation sample, \n",
    "* Average the goodness-of-fit measurement over the $S$ sections to determined the best tuning parameter. \n",
    "* In practice we can use  $S=5$ for 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Validation (time series data)\n",
    "\n",
    "* In time series context, cross validation must preserve the dependence structure. \n",
    "* If the time series is stationary, we can partition the data into $S$ consecutive blocks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "(i will skip this slide)\n",
    "\n",
    "* If the purpose is forecasting, then we can use nested CV. \n",
    "![ ](graph/CV_Figure.png)\n",
    "\n",
    "* Nested CV with fixed-length rolling window scheme\n",
    "* The sub-training data can also be an extending rolling window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection\n",
    "\n",
    "* Number of covariates $x_i$ can be large.\n",
    "\n",
    "* Conventional attitude: prior knowledge\n",
    "* Recently economists wake up from the long lasting negligence.\n",
    "    * Stock and Watson (2012): forecasting 143 US macroeconomic indicators.\n",
    "    * A horse race of several variable selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lasso\n",
    "\n",
    "* least-absolute-shrinkage-and-selection-operator\n",
    "(Lasso) (Tibshirani, 1996)\n",
    "* Penalizes the $L_1$ norm of the coefficients.\n",
    "The criterion function of Lasso is written as\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\Vert \\beta \\Vert_1\n",
    "$$\n",
    "where $\\lambda \\geq 0$ is a tuning parameter. \n",
    "\n",
    "Lasso shrinks some coefficients exactly to 0, in a wide range of values of $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![ ](graph/lasso_regression2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SCAD\n",
    "\n",
    "* Smoothly-clipped-absolute-deviation (SCAD) Fan and Li (2001):\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\sum_{j=1}^d \\rho_{\\lambda}( |\\beta_j| )\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\rho_{\\lambda}^{\\prime} (\\theta) = \\lambda \\left\\{ 1\\{\\theta\\leq \\lambda \\} +\n",
    "\\frac{(a\\lambda - \\theta)_+}{(a-1)\\lambda} \\cdot 1 \\{\\theta > \\lambda\\} \\right\\}\n",
    "$$\n",
    "for some $a>2$ and $\\theta>0$. \n",
    "\n",
    "* SCAD enjoys *oracle property*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adaptive Lasso\n",
    "\n",
    "*Adaptive Lasso* (Zou, 2006) also enjoys the oracle property.\n",
    "\n",
    "Two-step algorithm:\n",
    "1. First run a Lasso or ridge regression and save the estimator $\\hat{\\beta}^{(1)}$\n",
    "2. Solve \n",
    "$(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\sum_{j=1}^d  w_j |\\beta_j|$\n",
    "where $w_j = 1 /  |\\hat{\\beta}_j^{(1)} |^a$ and $a\\geq 1$ is a constant. (Common choice is $a = 1$ or 2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "R packages\n",
    "\n",
    "* `glmnet` or `LARS` implements Lasso\n",
    "* `ncvreg` carries out SCAD. \n",
    "* Adaptive Lasso by setting the weight via the argument `penalty.factor` in `glmnet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### My Works on Lasso\n",
    "\n",
    "* Shi (2016): GMM Lasso for IV\n",
    "* Lee, Shi and Gao (2022): Adaptive Lasso\n",
    "* Mei and Shi (2024): High-dimensional Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(magrittr)\n",
    "n <- 40\n",
    "p <- 50\n",
    "b0 <- c(rep(1, 10), rep(0, p - 10))\n",
    "x <- matrix(rnorm(n * p), n, p)\n",
    "y <- x %*% b0 + rnorm(n)\n",
    "\n",
    "ols <- MASS::ginv(t(x) %*% x) %*% (t(x) %*% y) # OLS\n",
    "# Implement Lasso by glmnet\n",
    "cv_lasso <- glmnet::cv.glmnet(x, y)\n",
    "lasso_result <- glmnet::glmnet(x, y, lambda = cv_lasso$lambda.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get weights\n",
    "b_temp <- as.numeric(lasso_result$beta)\n",
    "b_temp[b_temp == 0] <- 1e-8\n",
    "w <- 1 / abs(b_temp) # Let gamma = 1\n",
    "\n",
    "# Implement Adaptive Lasso by glmnet\n",
    "cv_alasso <- glmnet::cv.glmnet(x, y, penalty.factor = w)\n",
    "alasso_result <-\n",
    "  glmnet::glmnet(x, y, penalty.factor = w, lambda = cv_alasso$lambda.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot(b0, ylim = c(-0.8, 1.5), pch = 4, xlab = \"\", ylab = \"coefficient\")\n",
    "points(lasso_result$beta, col = \"red\", pch = 6)\n",
    "points(alasso_result$beta, col = \"blue\", pch = 5)\n",
    "points(ols, col = \"green\", pch = 3)\n",
    " \n",
    "# out of sample prediction\n",
    "x_new <- matrix(rnorm(n * p), n, p)\n",
    "y_new <- x_new %*% b0 + rnorm(n)\n",
    "lasso_msfe <- (y_new - predict(lasso_result, newx = x_new)) %>% var()\n",
    "alasso_msfe <- (y_new - predict(alasso_result, newx = x_new)) %>% var()\n",
    "ols_msfe <- (y_new - x_new %*% ols) %>% var()\n",
    "\n",
    "print(c(lasso_msfe, alasso_msfe, ols_msfe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DIY Lasso by `CVXR`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(CVXR)\n",
    "\n",
    "lambda <- 2 * cv_lasso$lambda.min # tuning parameter\n",
    "\n",
    "# CVXR for Lasso\n",
    "beta_cvxr <- Variable(p)\n",
    "obj <- sum_squares(y - x %*% beta_cvxr) / (2 * n) + lambda * p_norm(beta_cvxr, 1)\n",
    "prob <- Problem(Minimize(obj))\n",
    "lasso_cvxr <- solve(prob)\n",
    "beta_cvxr_hat <- lasso_cvxr$getValue(beta_cvxr) %>% as.vector() %>% print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stagewise Forward Selection\n",
    "\n",
    "More methods are available if prediction of the response variables is the sole purpose of the regression.\n",
    "\n",
    "Eg: *stagewise forward selection*\n",
    "\n",
    "1. Start from an empty model. \n",
    "2. Given many candidate $x_j$, in each round we add the regressor that can\n",
    "produce the biggest $R^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Close to the idea of *$L_2$ componentwise boosting*\n",
    "which does not adjust the coefficients fitted earlier\n",
    "\n",
    "* Shi and Huang (2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Second Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction-Oriented Methods\n",
    "\n",
    "* Methods that induces data-driven interaction of the covariates.\n",
    "* Interaction makes the covariates much more flexible\n",
    "* Insufficient theoretical understanding\n",
    "* \"Black-boxes\" methods\n",
    "\n",
    "* Surprisingly superior performance\n",
    "* Industry insiders are pondering \"alchemy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression Tree\n",
    "\n",
    "* Supervised learning: $x \\to y $\n",
    "* Regression tree (Breiman, 1984) recursively partitions the space of the regressors\n",
    "    * Each time a covariate is split into two dummies\n",
    "    * Splitting criterion is aggressive reduction of the SSR\n",
    "    * Tuning parameter is the depth of the tree\n",
    "    * Given a dataset $d$ and the depth of the tree, the fitted tree $\\hat{r}(d)$ is deterministic\n",
    "\n",
    "- Example: Using longitude and latitude for Beijing housing price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging\n",
    "\n",
    "* Tree is unstable\n",
    "* *Bootstrap averaging*, or *bagging*, reduces variance of trees (Breiman, 1996)\n",
    "    * Grow a tree for each bootstrap sample\n",
    "    * Simple average\n",
    "\n",
    "* An example of the *ensemble learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Inoue and Kilian (2008): an early application of bagging in time series forecast.\n",
    "* Hirano and Wright (2017): a theoretical perspective on the risk reduction of bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "* *Random forest* (Breiman, 2001):\n",
    "    * Draw a bootstrap sample\n",
    "    * Before each split, shakes up the regressors by randomly sampling $m$ out of the total $p$ covarites. Stop until the depth of the tree is reached.\n",
    "    * Average the trees over the bootstrap samples\n",
    "    \n",
    "* The tuning parameters are the tree depth and $m$\n",
    "* More stable than bagging thanks to \"de-correlation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(randomForest)\n",
    "require(MASS)#Package which contains the Boston housing dataset\n",
    "attach(Boston)\n",
    "set.seed(101)\n",
    "\n",
    "#training Sample with 300 observations\n",
    "train=sample(1:nrow(Boston),300)\n",
    "\n",
    "Boston.rf=randomForest(medv ~ . , data = Boston, subset = train)\n",
    "plot(Boston.rf)\n",
    "\n",
    "# getTree(Boston.rf)\n",
    "importance(Boston.rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Consistency of random forest is not proved\n",
    "until Scornet, Biau, and Vert (2015)\n",
    "* Inferential theory was first established by\n",
    "Wager Athey (2018)  in the context of treatment effect estimation\n",
    "* Athey, Tibshirani, and Wager (2019) generalizes CART to local maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "* Bagging and random forest use equal weight on each generated tree for the ensemble\n",
    "* Tree boosting takes a deterministic approach for the weights\n",
    "    1. Use the original data $d^0=(x_i,y_i)$ to grow a shallow tree $\\hat{r}^{0}(d^0)$. Save the prediction $f^0_i = \\alpha \\cdot \\hat{r}^0 (d^0, x_i)$ where\n",
    "   $\\alpha\\in [0,1]$ is a shrinkage tuning parameter. Save\n",
    "   the residual $e_i^{0} = y_i - f^0_i$. Set $m=1$.\n",
    "    2. In the $m$-th iteration, use the data $d^m = (x_i,e_i^{m-1})$ to grow a shallow tree $\\hat{r}^{m}(d^m)$. Save the prediction $f^m_i =  f^{m-1}_i +  \\alpha \\cdot \\hat{r}^m (d, x_i)$. Save\n",
    "   the residual $e_i^{m} = y_i - f^m_i$. Update $m = m+1$.\n",
    "    3. Repeat Step 2 until $m > M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Boosting has three tuning parameters: the tree depth,  the shrinkage level $\\alpha$, and the number of iterations $M$\n",
    "* The algorithm can be sensitive to any of the three tuning parameters\n",
    "* When a model is tuned well, it can performs remarkably\n",
    "    * Example: Beijing housing data.\n",
    "    * Gradient boosting via the package `gbm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Statisticians view boosting as a gradient descent algorithm to reduce the risk. The fitted\n",
    "tree in each iteration is the deepest descent direction, while the shrinkage tames the fitting to avoid proceeding too aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(caret)\n",
    "\n",
    "\n",
    "load(\"data_example/lianjia.RData\")\n",
    "N <- nrow(lianjia) # a smaller sample\n",
    "lianjia <- lianjia[base::sample(1:N, round(N * 0.05 )), ]\n",
    "\n",
    "train_ind <- caret::createDataPartition(1:nrow(lianjia), p = 0.1)$Resample1\n",
    "# p = 0.1 to save time. Better to use p = 0.75\n",
    "\n",
    "gbmGrid <- expand.grid(\n",
    "  interaction.depth = seq(from = 10, to = 50, by = 30),\n",
    "  n.trees = seq(from = 1000, to = 10000, by = 4000),\n",
    "  shrinkage = c(0.01),\n",
    "  n.minobsinnode = 20\n",
    ")\n",
    "\n",
    "gbmControl <- caret::trainControl(method = \"cv\", number = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "formula.GBM <- price ~\n",
    "  square + livingRoom + drawingRoom + kitchen + bathRoom +\n",
    "  floor_type + floor_total + elevator + ladderRatio +\n",
    "  age + DOM + followers + fiveYearsProperty +\n",
    "  subway + district + Lng + Lat + t_trade +\n",
    "  communityAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(doParallel)\n",
    "library(gbm)\n",
    "\n",
    "gbmControl=trainControl(method=\"repeatedcv\",number=5,repeats=1)\n",
    "\n",
    "registerDoParallel(8)\n",
    "t=Sys.time()\n",
    "boostingReg=train(formula.GBM, \n",
    "                  data=lianjia[train_ind,],\n",
    "                  method=\"gbm\",\n",
    "                  distribution=\"gaussian\",\n",
    "                  trControl=gbmControl,\n",
    "                  tuneGrid=gbmGrid,\n",
    "                  metric=\"Rsquared\",\n",
    "                  verbose=F)\n",
    "cat(\"Time Cost of Finding Best Tuning Parameters:\",Sys.time()-t,\"\\n\")\n",
    "doParallel::stopImplicitCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "gbmTune = boostingReg$bestTune\n",
    "cat(\"The best tuning parameters for GBM are: \\n\");\n",
    "print(gbmTune)\n",
    "\n",
    "pred.boosting=predict(boostingReg,newdata=lianjia[-train_ind,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lmReg=lm(formula.GBM, data=lianjia[train_ind,])\n",
    "pred.lm=predict(lmReg,newdata=lianjia[-train_ind,])\n",
    "\n",
    "\n",
    "# Comparison\n",
    "\n",
    "target=lianjia[-train_ind,]$price\n",
    "cat(\"R-squared of GBM prediction =\",miscTools::rSquared(target,target-pred.boosting),\"\\n\")\n",
    "cat(\"R-squared of LM prediction =\",miscTools::rSquared(target,target-pred.lm),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Many variants of boosting algorithms\n",
    "    * $L_2$-boosting\n",
    "    * componentwise boosting\n",
    "    * AdaBoosting, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* Mature algorithms for implementation\n",
    "* Theoretical investigation is in progress\n",
    "* Economic applications are ongoing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
