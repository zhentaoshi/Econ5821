{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "### Zhentao Shi\n",
    "\n",
    "![NN](graph/Colored_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The workhorse of AI\n",
    "\n",
    "## Types of Neural Networks\n",
    "\n",
    "\n",
    "* Feedforward (FNN)\n",
    "* Convolution (CNN)\n",
    "* Recurrent (RNN)\n",
    "* Long short-term memory (LSTM)\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layers\n",
    "\n",
    "* The transition from layer $k-1$ to layer $k$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_l^{(k)} & = b_{l0}^{(k-1)} + \\sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\\\ \n",
    "a_l^{(k)} & = \\sigma ( z_l^{(k)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $a_j^{(0)} = x_j$ is the input.\n",
    "\n",
    "* The latent variable $z_l^{(k)}$ usually takes a linear form\n",
    "* *Activation function* $\\sigma(\\cdot)$ is usually a simple nonlinear function\n",
    "* Popular choices\n",
    "  * Sigmoid: $1/(1+\\exp(-x))$\n",
    "  * Rectified linear unit (ReLu) $z\\cdot 1\\{x\\geq 0\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Does It Work?\n",
    "\n",
    "* Animated video by [3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "\n",
    "* Feedforward: criterion evaluation\n",
    "* Back propagation: parameter adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization\n",
    "\n",
    "* One-layer feedforward NN for demonstration\n",
    "* Input: $p$\n",
    "* Hidden nodes: $K$\n",
    "  \n",
    "* Criterion: \n",
    "$$\n",
    "\\min_{\\theta}   \\frac{1}{2}\\sum_{i=1}^n  Q_i \\textrm{ where } Q_i = [y_i - \\hat{y}_i ]^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Components in Optimization\n",
    "\n",
    "* Input -> hidden layer is indexed as (1), \n",
    "* The hidden -> output layer as (2):\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}_i & =  \\beta^{(2)} + \\sum_{j=1}^K w_{j}^{(2)} z_j^{(2)} \\\\ \n",
    "z_j^{(2)} & = \\sigma \\left( z^{(1)}_j\\right) \\\\\n",
    "z_j^{(1)} & =\\beta_j^{(1)} + \\sum_{\\ell=1}^p w_{j\\ell}^{(1)} x_{i} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Intercept is called **bias**\n",
    "* Slope coefficient is called **weight**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient method\n",
    "\n",
    "Taylor expansion\n",
    "\n",
    "$$\n",
    "Q(\\theta_{t+1}) = Q(\\theta_t) +  \\nabla^{\\top} Q(\\theta_t) (\\theta_{t+1}-\\theta_{t}) + h.o.t.\n",
    "$$\n",
    "where\n",
    "* $\\nabla Q(\\theta_t)$ is **Gradient**\n",
    "* $(\\theta_{t+1}-\\theta_{t})$ is unknown, use $p_t$ (**length of step**) to replace it as\n",
    "$$\n",
    "Q(\\theta_{t+1}) = Q(\\theta_t) +  \\nabla^{\\top} Q(\\theta_t) p_t\n",
    "$$\n",
    "which direction reduces the value of function?\n",
    "\n",
    "* Choose $p_t = - \\alpha \\cdot \\nabla Q(\\theta_t)$ ensures reduction in $Q$, where $\\alpha$ is the **learning rate**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Output layer -> hidden layer\n",
    "\\begin{align*}\n",
    "\\frac{\\partial Q_{i}}{\\partial\\beta^{(2)}} & =-\\left[y_{i}-f^{(2)}\\left(X_{i}\\right)\\right]\\\\\n",
    "\\frac{\\partial Q_{i}}{\\partial w_{j}^{(2)}} & =-\\left[y_{i}-f^{(2)}\\left(X_{i}\\right)\\right]\\sigma\\left(z_{j}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "* Hidden layer -> input layer: \n",
    "  * NN is a composite function. By the chain rule \n",
    "\\begin{align*}\n",
    "\\frac{\\partial Q_{i}}{\\partial\\beta^{(1)}} & =\\frac{\\partial Q_{i}}{\\partial\\beta^{(2)}}\\cdot\\sigma'\\left(z_{j}\\right)\\\\\n",
    "\\frac{\\partial Q_{i}}{\\partial w_{j}^{(1)}} & =\\frac{\\partial Q_{i}}{\\partial w_{j}^{(2)}}\\cdot\\sigma'\\left(z_{j}\\right)x_{i}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example**: Use NN to fit a linear model.\n",
    "* Notice $x = \\mathrm{ReLu}(x) - \\mathrm{ReLu}(-x)$. A linear function can be exactly represented by NN with ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# simulate data\n",
    "np.random.seed(1)  # For reproducibility\n",
    "n = 100\n",
    "x = np.random.rand(n, 2)\n",
    "y = 1 + 2 * x[:, 0] + 1 * x[:, 1] + np.random.randn(n)  # Linear relationship with noise\n",
    "y = y.reshape(-1, 1)  # Reshape to column vector (n, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Neural Network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"Initialize weights and biases.\"\"\"\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01  # Input to hidden weights\n",
    "        self.b1 = np.zeros((1, hidden_size))  # bias\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01  # Hidden to output weights\n",
    "        self.b2 = np.zeros((1, output_size))  # bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Compute the forward pass.\"\"\"\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1  # Input to hidden layer\n",
    "        self.A1 = np.maximum(0, self.Z1)  # ReLU activation\n",
    "\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2  # Hidden to output layer\n",
    "        self.A2 = self.Z2  # Linear activation (identity)\n",
    "        return self.A2\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calculate Mean Squared Error loss.\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Compute gradients using backpropagation.\"\"\"\n",
    "        n = X.shape[0]  # sample size\n",
    "\n",
    "        # Output layer gradients. 2nd layer is computed first.\n",
    "        dZ2 = 2 * (self.A2 - y) / n  # Gradient of loss w.r.t. Z2\n",
    "        \n",
    "        dW2 = np.dot(self.A1.T, dZ2)  # Gradient of loss w.r.t. W2\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True)  # Gradient of loss w.r.t. b2\n",
    "\n",
    "        # Hidden layer gradients. 1st layer is computed last.\n",
    "        dA1 = np.dot(dZ2, self.W2.T)  # Gradient of loss w.r.t. A1\n",
    "        dZ1 = dA1 * (self.Z1 > 0)  # Gradient of loss w.r.t. Z1 (ReLU derivative)\n",
    "        dW1 = np.dot(X.T, dZ1)  # Gradient of loss w.r.t. W1\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True)  # Gradient of loss w.r.t. b1\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def update(self, dW1, db1, dW2, db2, learning_rate):\n",
    "        \"\"\"Update weights and biases using gradients.\"\"\"\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        \"\"\"Train the neural network.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)  # Forward pass\n",
    "            loss = self.compute_loss(y, y_pred)  # Compute loss\n",
    "            if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y)  # Backward pass\n",
    "            self.update(dW1, db1, dW2, db2, learning_rate)  # Update parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 7.843827\n",
      "Epoch 100, Loss: 1.589406\n",
      "Epoch 200, Loss: 1.461798\n",
      "Epoch 300, Loss: 1.415684\n",
      "Epoch 400, Loss: 1.347697\n",
      "Epoch 500, Loss: 1.262448\n",
      "Epoch 600, Loss: 1.181384\n",
      "Epoch 700, Loss: 1.124319\n",
      "Epoch 800, Loss: 1.090392\n",
      "Epoch 900, Loss: 1.071762\n",
      "Final Loss: 1.061774\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the neural network\n",
    "    nn = NeuralNetwork(input_size=2, hidden_size=8, output_size=1)\n",
    "    \n",
    "    # Train the network\n",
    "    nn.train(x, y, epochs=1000, learning_rate=0.01)\n",
    "    \n",
    "    # Evaluate the final performance\n",
    "    final_pred = nn.forward(x)\n",
    "    final_loss = nn.compute_loss(y, final_pred)\n",
    "    print(f\"Final Loss: {final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A `pytorch` implementation of the same neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Neural Network class\n",
    "class NeuralNetwork_torch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork_torch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 8.963584\n",
      "Epoch 100, Loss: 1.196773\n",
      "Epoch 200, Loss: 1.137686\n",
      "Epoch 300, Loss: 1.098162\n",
      "Epoch 400, Loss: 1.073694\n",
      "Epoch 500, Loss: 1.059587\n",
      "Epoch 600, Loss: 1.051506\n",
      "Epoch 700, Loss: 1.046908\n",
      "Epoch 800, Loss: 1.044397\n",
      "Epoch 900, Loss: 1.043045\n",
      "Final Loss: 1.042303\n"
     ]
    }
   ],
   "source": [
    " # Convert numpy arrays to torch tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Initialize the neural network\n",
    "input_size = x.shape[1]\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "model = NeuralNetwork_torch(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the neural network\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(x_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Evaluate the final performance\n",
    "final_pred = model(x_tensor).detach().numpy()\n",
    "final_loss = criterion(torch.tensor(final_pred), y_tensor).item()\n",
    "print(f\"Final Loss: {final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "* Large $n$\n",
    "* Sample a *minibatch*\n",
    "  * Unbiased gradient, but large variance\n",
    "* Learning rate\n",
    "* Many epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "* $L_1$-norm (Lasso)\n",
    "* $L_2$-norm (ridge)\n",
    "* Learning rate\n",
    "* Number of epochs and minibatches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulation Example\n",
    "\n",
    "* Use NN to solve Poisson regression\n",
    "  * A trivial example for demonstration\n",
    "  * No hidden layer\n",
    "  * Keep the essence\n",
    "  \n",
    "* See `data_example/nn_torch.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Network Structures\n",
    "\n",
    "* Time series\n",
    "  * Recurrent NN (RNN)\n",
    "  * Long short-term memory (LSTM) (See `data_example/nn_LSTM.ipynb`)\n",
    "* Graphics\n",
    "  * Convolutional NN (CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.497974\n",
      "Epoch 100, Loss: 0.467624\n",
      "Epoch 200, Loss: 0.463342\n",
      "Epoch 300, Loss: 0.462317\n",
      "Epoch 400, Loss: 0.461109\n",
      "Epoch 500, Loss: 0.458347\n",
      "Epoch 600, Loss: 0.451663\n",
      "Epoch 700, Loss: 0.435664\n",
      "Epoch 800, Loss: 0.398754\n",
      "Epoch 900, Loss: 0.321377\n",
      "Final Loss: 0.194452\n",
      "\n",
      "Sample Predictions vs Targets:\n",
      "Predicted: 0.1906, Target: 0.0536\n",
      "Predicted: 0.1661, Target: 0.0633\n",
      "Predicted: 0.1653, Target: 0.1440\n",
      "Predicted: 0.1913, Target: -0.0616\n",
      "Predicted: 0.1289, Target: -0.0328\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic time series data (sine wave with noise)\n",
    "def generate_data(n_samples, seq_length):\n",
    "    t = np.linspace(0, 10, n_samples)\n",
    "    data = np.sin(t) + np.random.normal(0, 0.1, n_samples)  # Sine wave + noise\n",
    "    X, y = [], []\n",
    "    for i in range(n_samples - seq_length):\n",
    "        X.append(data[i:i + seq_length])  # Input sequence\n",
    "        y.append(data[i + seq_length])    # Next value to predict\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)\n",
    "\n",
    "# Define RNN class\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"Initialize weights and biases.\"\"\"\n",
    "        # Weight matrices\n",
    "        self.Wxh = np.random.randn(input_size, hidden_size) * 0.01  # Input to hidden\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
    "        self.Why = np.random.randn(hidden_size, output_size) * 0.01  # Hidden to output\n",
    "        # Biases\n",
    "        self.bh = np.zeros((1, hidden_size))  # Hidden bias\n",
    "        self.by = np.zeros((1, output_size))  # Output bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the RNN.\"\"\"\n",
    "        seq_length = X.shape[1]\n",
    "        batch_size = X.shape[0]\n",
    "        hidden_size = self.Whh.shape[0]\n",
    "        \n",
    "        # Initialize hidden state and storage for activations\n",
    "        h = np.zeros((batch_size, hidden_size))\n",
    "        self.hs = [h]  # Store hidden states for backprop\n",
    "        self.xs = [X[:, t, :] for t in range(seq_length)]  # Store inputs\n",
    "        \n",
    "        # Forward pass over time steps\n",
    "        for t in range(seq_length):\n",
    "            h = np.tanh(np.dot(X[:, t, :], self.Wxh) + np.dot(h, self.Whh) + self.bh)\n",
    "            self.hs.append(h)\n",
    "        \n",
    "        # Output layer (at the last time step)\n",
    "        y = np.dot(h, self.Why) + self.by\n",
    "        return y\n",
    "\n",
    "    def backward(self, X, y_true, y_pred, learning_rate):\n",
    "        \"\"\"Backpropagation through time (BPTT).\"\"\"\n",
    "        seq_length = X.shape[1]\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dy = y_pred - y_true  # Gradient of loss (MSE) w.r.t. output\n",
    "        dWhy = np.dot(self.hs[-1].T, dy) / batch_size  # Gradient w.r.t. Why\n",
    "        dby = np.sum(dy, axis=0, keepdims=True) / batch_size  # Gradient w.r.t. by\n",
    "        \n",
    "        # Initialize hidden state gradient\n",
    "        dh_next = np.dot(dy, self.Why.T)  # Gradient from output to last hidden state\n",
    "        \n",
    "        # Backprop through time\n",
    "        for t in range(seq_length - 1, -1, -1):\n",
    "            dh_raw = dh_next * (1 - self.hs[t + 1] ** 2)  # Gradient w.r.t. pre-activation (tanh derivative)\n",
    "            dbh += np.sum(dh_raw, axis=0, keepdims=True) / batch_size\n",
    "            dWxh += np.dot(self.xs[t].T, dh_raw) / batch_size\n",
    "            dWhh += np.dot(self.hs[t].T, dh_raw) / batch_size\n",
    "            dh_next = np.dot(dh_raw, self.Whh.T)  # Propagate gradient to previous time step\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.Wxh -= learning_rate * dWxh\n",
    "        self.Whh -= learning_rate * dWhh\n",
    "        self.Why -= learning_rate * dWhy\n",
    "        self.bh -= learning_rate * dbh\n",
    "        self.by -= learning_rate * dby\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        \"\"\"Train the RNN.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = np.mean((y_pred - y) ** 2)  # Mean Squared Error\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "            self.backward(X, y, y_pred, learning_rate)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate data\n",
    "    n_samples = 1000\n",
    "    seq_length = 10\n",
    "    X, y = generate_data(n_samples, seq_length)\n",
    "    X = X.reshape(-1, seq_length, 1)  # Shape: (samples, timesteps, features)\n",
    "    \n",
    "    # Initialize and train RNN\n",
    "    rnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n",
    "    rnn.train(X, y, epochs=1000, learning_rate=0.01)\n",
    "    \n",
    "    # Test the model\n",
    "    y_pred = rnn.forward(X)\n",
    "    final_loss = np.mean((y_pred - y) ** 2)\n",
    "    print(f\"Final Loss: {final_loss:.6f}\")\n",
    "    \n",
    "    # Optional: Print some predictions\n",
    "    print(\"\\nSample Predictions vs Targets:\")\n",
    "    for i in range(5):\n",
    "        print(f\"Predicted: {y_pred[i][0]:.4f}, Target: {y[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.708658\n",
      "Epoch 100, Loss: 0.013469\n",
      "Epoch 200, Loss: 0.012991\n",
      "Epoch 300, Loss: 0.012638\n",
      "Epoch 400, Loss: 0.012350\n",
      "Epoch 500, Loss: 0.012142\n",
      "Epoch 600, Loss: 0.011987\n",
      "Epoch 700, Loss: 0.011860\n",
      "Epoch 800, Loss: 0.011754\n",
      "Epoch 900, Loss: 0.011671\n",
      "Final Loss: 0.011614\n",
      "\n",
      "Sample Predictions vs Targets:\n",
      "Predicted: 0.0969, Target: 0.0536\n",
      "Predicted: 0.0838, Target: 0.0633\n",
      "Predicted: 0.0803, Target: 0.1440\n",
      "Predicted: 0.0931, Target: -0.0616\n",
      "Predicted: 0.0501, Target: -0.0328\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the RNN class\n",
    "class RNN_torch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_torch, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)  # Initial hidden state\n",
    "        out, _ = self.rnn(x, h0)  # RNN forward pass\n",
    "        out = self.fc(out[:, -1, :])  # Fully connected layer on the last time step\n",
    "        return out\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Initialize the RNN\n",
    "input_size = X.shape[2]\n",
    "hidden_size = 5\n",
    "output_size = 1\n",
    "model = RNN_torch(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the RNN\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Evaluate the final performance\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred = model(X_tensor).numpy()\n",
    "    final_loss = criterion(torch.tensor(final_pred), y_tensor).item()\n",
    "    print(f\"Final Loss: {final_loss:.6f}\")\n",
    "\n",
    "# Optional: Print some predictions\n",
    "print(\"\\nSample Predictions vs Targets:\")\n",
    "for i in range(5):\n",
    "    print(f\"Predicted: {final_pred[i][0]:.4f}, Target: {y[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Theory is Incomplete\n",
    "\n",
    "* Theoretical understanding is an ongoing endeavor.\n",
    "* Hornik, Stinchcombe, and White (1989):\n",
    "  * A single hidden layer neural network, given enough many nodes, is a *universal approximator* for any measurable function.\n",
    "* Deep learning: engineering breakthrough\n",
    "* Big data available"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernel_info": {
   "name": "ir"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
