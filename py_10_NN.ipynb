{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "### Zhentao Shi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks\n",
    "\n",
    "* Neural network is the workhorse of AI\n",
    "* A type of nonlinear models (with a structure)\n",
    "\n",
    "![NN](graph/Colored_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layers\n",
    "\n",
    "* The transition from layer $k-1$ to layer $k$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_l^{(k)} & = b_{l0}^{(k-1)} + \\sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\\\ \n",
    "a_l^{(k)} & = g^{(k)} ( z_l^{(k)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $a_j^{(0)} = x_j$ is the input.\n",
    "\n",
    "* The latent variable $z_l^{(k)}$ usually takes a linear form\n",
    "* *Activation function* $g(\\cdot)$ is usually a simple nonlinear function\n",
    "* Popular choices\n",
    "  * Sigmoid: ($1/(1+\\exp(-x))$)\n",
    "  * Rectified linear unit (ReLu) $z\\cdot 1\\{x\\geq 0\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Does It Work?\n",
    "\n",
    "* Animated video by [3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "\n",
    "* Feedforward: criterion evaluation\n",
    "* Back propagation: parameter adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization\n",
    "\n",
    "* One-layer feedforward NN for demonstration\n",
    "* Input: $p$\n",
    "* Hidden nodes: $K$\n",
    "  \n",
    "* Criterion: \n",
    "$$\n",
    "\\min_{\\theta}   \\frac{1}{2}\\sum_{i=1}^n  Q_i \\textrm{ where } Q_i = [y_i - f^{(2)}(X_i) ]^2\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "f^{(2)}(X_i) & =  \\beta^{(2)} + \\sum_{j=1}^K w_{j}^{(2)} \\sigma \\left( z_j\\right) \\\\\n",
    "z_j & =\\beta_j^{(1)} + \\sum_{\\ell=1}^p w_{j\\ell}^{(1)} x_{i} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient method\n",
    "\n",
    "Taylor expansion\n",
    "\n",
    "$$\n",
    "Q(\\theta_{t+1}) = Q(\\theta_t) +  \\nabla^{\\top} Q(\\theta_t) (\\theta_{t+1}-\\theta_{t}) + h.o.t.\n",
    "$$\n",
    "where\n",
    "* $\\nabla Q(\\theta_t)$ is **Gradient**\n",
    "* $(\\theta_{t+1}-\\theta_{t})$ is unknown, use $p_t$ (**length of step**) to replace it as\n",
    "$$\n",
    "Q(\\theta_{t+1}) = Q(\\theta_t) +  \\nabla^{\\top} Q(\\theta_t) p_t\n",
    "$$\n",
    "* Choose $p_k = - \\alpha \\cdot \\nabla Q(\\theta_t)$ ensures reduction in $Q$, where $\\alpha$ is the **learning rate**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Output layer -> hidden layer\n",
    "\\begin{align*}\n",
    "\\frac{\\partial Q_{i}}{\\partial\\beta^{(2)}} & =-\\left[y_{i}-f^{(2)}\\left(X_{i}\\right)\\right]\\\\\n",
    "\\frac{\\partial Q_{i}}{\\partial w_{j}^{(2)}} & =-\\left[y_{i}-f^{(2)}\\left(X_{i}\\right)\\right]\\sigma\\left(z_{j}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "* Hidden layer -> input layer: by the chain rule \n",
    "\\begin{align*}\n",
    "\\frac{\\partial Q_{i}}{\\partial\\beta^{(1)}} & =\\frac{\\partial Q_{i}}{\\partial\\beta^{(2)}}\\cdot\\sigma'\\left(z_{j}\\right)\\\\\n",
    "\\frac{\\partial Q_{i}}{\\partial w_{j}^{(1)}} & =\\frac{\\partial Q_{i}}{\\partial w_{j}^{(2)}}\\cdot\\sigma'\\left(z_{j}\\right)x_{i}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "* Large n\n",
    "* Sample a *minibatch*\n",
    "  * Unbiased gradient, but large variance\n",
    "* Learning rate\n",
    "* Many epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "* $L_1$-norm (Lasso)\n",
    "* $L_2$-norm (ridge)\n",
    "* Learning rate\n",
    "* Number of epochs and minibatches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frameworks\n",
    "\n",
    "* Google's `Tensorflow`\n",
    "  * Keras: high level, easy to implement\n",
    "* Meta's `pytorch`\n",
    "  * Literal style\n",
    "  * Easy to use/reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulation Example\n",
    "\n",
    "* Use NN to solve Poisson regression\n",
    "  * A trivial example for demonstration\n",
    "  * No hidden layer\n",
    "  * Keep the essence\n",
    "  \n",
    "* See `data_example/nn_Poisson_Keras_HD.ipynb`\n",
    "* See `data_example/nn_torch.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Network Structures\n",
    "\n",
    "* Time series\n",
    "  * Recurrent NN (RNN)\n",
    "  * Long term and short term memory (LSTM) (See `data_example/nn_LSTM.ipynb`)\n",
    "* Graphics\n",
    "  * Convolutional NN (CNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Theory is Incomplete\n",
    "\n",
    "* Theoretical understanding is an ongoing endeavor.\n",
    "* Hornik, Stinchcombe, and White (1989):\n",
    "  * A single hidden layer neural network, given enough many nodes, is a *universal approximator* for any measurable function.\n",
    "* Deep learning: engineering breakthrough\n",
    "* Big data available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- ## Reinforcement Learning\n",
    "\n",
    "* Policy function $d(x_t; \\theta)$, where $\\theta$ is the parameter\n",
    "* Response $y_t$, with reward $r( d (x_t; \\theta), y_t)$\n",
    "* Optimal invariant parameter\n",
    "$$\n",
    "\\theta^* = \\arg \\max_{\\theta} \\sum_{t=1}^T r( d(x_t; \\theta), y_t)\n",
    "$$\n",
    "\n",
    "* Regret: \n",
    "\n",
    "$$\n",
    " \\sum_{t=1}^T [ r( d(x_t; \\theta^*), y_t) - r( d(x_t; \\theta_t), y_t)] \n",
    "$$ -->"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "rise": {
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
