{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "### Zhentao Shi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks\n",
    "\n",
    "* Neural network is the workhorse of AI\n",
    "* A type of nonlinear models (with a structure)\n",
    "\n",
    "![NN](graph/Colored_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layers\n",
    "\n",
    "* The transition from layer $k-1$ to layer $k$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_l^{(k)} & = b_{l0}^{(k-1)} + \\sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\\\ \n",
    "a_l^{(k)} & = \\sigma ( z_l^{(k)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $a_j^{(0)} = x_j$ is the input.\n",
    "\n",
    "* The latent variable $z_l^{(k)}$ usually takes a linear form\n",
    "* *Activation function* $\\sigma(\\cdot)$ is usually a simple nonlinear function\n",
    "* Popular choices\n",
    "  * Sigmoid: $1/(1+\\exp(-x))$\n",
    "  * Rectified linear unit (ReLu) $z\\cdot 1\\{x\\geq 0\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Does It Work?\n",
    "\n",
    "* Animated video by [3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "\n",
    "* Feedforward: criterion evaluation\n",
    "* Back propagation: parameter adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization\n",
    "\n",
    "* One-layer feedforward NN for demonstration\n",
    "* Input: $p$\n",
    "* Hidden nodes: $K$\n",
    "  \n",
    "* Criterion: \n",
    "$$\n",
    "\\min_{\\theta}   \\frac{1}{2}\\sum_{i=1}^n  Q_i \\textrm{ where } Q_i = [y_i - \\hat{y}_i ]^2\n",
    "$$\n",
    "where the input -> hidden layer is indexed as (1), and the hidden -> output layer as (2):\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}_i & =  \\beta^{(2)} + \\sum_{j=1}^K w_{j}^{(2)} z_j^{(2)} \\\\ \n",
    "z_j^{(2)} & = \\sigma \\left( z^{(1)}_j\\right) \\\\\n",
    "z_j^{(1)} & =\\beta_j^{(1)} + \\sum_{\\ell=1}^p w_{j\\ell}^{(1)} x_{i} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Intercept is called **bias**\n",
    "* Slope coefficient is called **weight**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Use NN to fit a linear model.\n",
    "* Notice $x = \\mathrm{ReLu}(x) - \\mathrm{ReLu}(-x)$. A linear function can be exactly represented by NN with ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# simulate data\n",
    "np.random.seed(1)  # For reproducibility\n",
    "n = 100\n",
    "x = np.random.rand(n, 2)\n",
    "y = 1 + 2 * x[:, 0] + 1 * x[:, 1] + np.random.randn(n)  # Linear relationship with noise\n",
    "y = y.reshape(-1, 1)  # Reshape to column vector (n, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 7.844266\n",
      "Epoch 100, Loss: 1.605901\n",
      "Epoch 200, Loss: 1.485044\n",
      "Epoch 300, Loss: 1.455069\n",
      "Epoch 400, Loss: 1.405080\n",
      "Epoch 500, Loss: 1.331050\n",
      "Epoch 600, Loss: 1.243934\n",
      "Epoch 700, Loss: 1.169365\n",
      "Epoch 800, Loss: 1.118070\n",
      "Epoch 900, Loss: 1.087532\n",
      "Final Loss: 1.070607\n"
     ]
    }
   ],
   "source": [
    "# Define the Neural Network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"Initialize weights and biases.\"\"\"\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01  # Input to hidden weights\n",
    "        self.b1 = np.zeros((1, hidden_size))  # bias\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01  # Hidden to output weights\n",
    "        self.b2 = np.zeros((1, output_size))  # bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Compute the forward pass.\"\"\"\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1  # Input to hidden layer\n",
    "        self.A1 = np.maximum(0, self.Z1)  # ReLU activation\n",
    "\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2  # Hidden to output layer\n",
    "        self.A2 = self.Z2  # Linear activation (identity)\n",
    "        return self.A2\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calculate Mean Squared Error loss.\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Compute gradients using backpropagation.\"\"\"\n",
    "        n = X.shape[0]  # sample size\n",
    "\n",
    "        # Output layer gradients. 2nd layer is computed first.\n",
    "        dZ2 = 2 * (self.A2 - y) / n  # Gradient of loss w.r.t. Z2\n",
    "        \n",
    "        dW2 = np.dot(self.A1.T, dZ2)  # Gradient of loss w.r.t. W2\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True)  # Gradient of loss w.r.t. b2\n",
    "\n",
    "        # Hidden layer gradients. 1st layer is computed last.\n",
    "        dA1 = np.dot(dZ2, self.W2.T)  # Gradient of loss w.r.t. A1\n",
    "        dZ1 = dA1 * (self.Z1 > 0)  # Gradient of loss w.r.t. Z1 (ReLU derivative)\n",
    "        dW1 = np.dot(X.T, dZ1)  # Gradient of loss w.r.t. W1\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True)  # Gradient of loss w.r.t. b1\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def update(self, dW1, db1, dW2, db2, learning_rate):\n",
    "        \"\"\"Update weights and biases using gradients.\"\"\"\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        \"\"\"Train the neural network.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)  # Forward pass\n",
    "            loss = self.compute_loss(y, y_pred)  # Compute loss\n",
    "            if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y)  # Backward pass\n",
    "            self.update(dW1, db1, dW2, db2, learning_rate)  # Update parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the neural network\n",
    "    nn = NeuralNetwork(input_size=2, hidden_size=8, output_size=1)\n",
    "    \n",
    "    # Train the network\n",
    "    nn.train(x, y, epochs=1000, learning_rate=0.01)\n",
    "    \n",
    "    # Evaluate the final performance\n",
    "    final_pred = nn.forward(x)\n",
    "    final_loss = nn.compute_loss(y, final_pred)\n",
    "    print(f\"Final Loss: {final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `pytorch` implementation of the same neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 7.346087\n",
      "Epoch 100, Loss: 1.207268\n",
      "Epoch 200, Loss: 1.138949\n",
      "Epoch 300, Loss: 1.096048\n",
      "Epoch 400, Loss: 1.070429\n",
      "Epoch 500, Loss: 1.056140\n",
      "Epoch 600, Loss: 1.048691\n",
      "Epoch 700, Loss: 1.044980\n",
      "Epoch 800, Loss: 1.043123\n",
      "Epoch 900, Loss: 1.042143\n",
      "Final Loss: 1.041657\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Neural Network class\n",
    "class NeuralNetwork_torch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork_torch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Initialize the neural network\n",
    "input_size = x.shape[1]\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "model = NeuralNetwork_torch(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the neural network\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(x_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Evaluate the final performance\n",
    "final_pred = model(x_tensor).detach().numpy()\n",
    "final_loss = criterion(torch.tensor(final_pred), y_tensor).item()\n",
    "print(f\"Final Loss: {final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient method\n",
    "\n",
    "Taylor expansion\n",
    "\n",
    "$$\n",
    "Q(\\theta_{t+1}) = Q(\\theta_t) +  \\nabla^{\\top} Q(\\theta_t) (\\theta_{t+1}-\\theta_{t}) + h.o.t.\n",
    "$$\n",
    "where\n",
    "* $\\nabla Q(\\theta_t)$ is **Gradient**\n",
    "* $(\\theta_{t+1}-\\theta_{t})$ is unknown, use $p_t$ (**length of step**) to replace it as\n",
    "$$\n",
    "Q(\\theta_{t+1}) = Q(\\theta_t) +  \\nabla^{\\top} Q(\\theta_t) p_t\n",
    "$$\n",
    "which direction reduces the value of function?\n",
    "\n",
    "* Choose $p_t = - \\alpha \\cdot \\nabla Q(\\theta_t)$ ensures reduction in $Q$, where $\\alpha$ is the **learning rate**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Output layer -> hidden layer\n",
    "\\begin{align*}\n",
    "\\frac{\\partial Q_{i}}{\\partial\\beta^{(2)}} & =-\\left[y_{i}-f^{(2)}\\left(X_{i}\\right)\\right]\\\\\n",
    "\\frac{\\partial Q_{i}}{\\partial w_{j}^{(2)}} & =-\\left[y_{i}-f^{(2)}\\left(X_{i}\\right)\\right]\\sigma\\left(z_{j}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "* Hidden layer -> input layer: \n",
    "  * NN is a composite function. By the chain rule \n",
    "\\begin{align*}\n",
    "\\frac{\\partial Q_{i}}{\\partial\\beta^{(1)}} & =\\frac{\\partial Q_{i}}{\\partial\\beta^{(2)}}\\cdot\\sigma'\\left(z_{j}\\right)\\\\\n",
    "\\frac{\\partial Q_{i}}{\\partial w_{j}^{(1)}} & =\\frac{\\partial Q_{i}}{\\partial w_{j}^{(2)}}\\cdot\\sigma'\\left(z_{j}\\right)x_{i}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Neural Network Setup\n",
    "\n",
    "Consider a neural network with $L$ layers, where:\n",
    "\n",
    "- Layer $l = 1$ is the input layer, and $l = L$ is the output layer.\n",
    "- The input is denoted as $\\mathbf{a}^{(0)} = \\mathbf{x}$, the activation of the input layer.\n",
    "- For each layer $l = 1, 2, \\dots, L$:\n",
    "  - $\\mathbf{W}^{(l)}$ is the weight matrix connecting layer $l-1$ to layer $l$.\n",
    "  - $\\mathbf{b}^{(l)}$ is the bias vector for layer $l$.\n",
    "  - $\\mathbf{z}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}$ is the pre-activation (weighted input) for layer $l$.\n",
    "  - $\\mathbf{a}^{(l)} = \\sigma^{(l)}(\\mathbf{z}^{(l)})$ is the activation of layer $l$, where $\\sigma^{(l)}$ is the activation function (e.g., sigmoid, ReLU, or linear) applied element-wise.\n",
    "- The output of the network is $\\mathbf{a}^{(L)}$, and the loss function is $L = \\mathcal{L}(\\mathbf{a}^{(L)}, \\mathbf{t})$, where $\\mathbf{t}$ is the target (true label).\n",
    "\n",
    "The goal of backpropagation is to compute the gradients $\\frac{\\partial L}{\\partial \\mathbf{W}^{(l)}}$ and $\\frac{\\partial L}{\\partial \\mathbf{b}^{(l)}}$ for each layer $l$, which are used to update the weights and biases during training.\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation Algorithm\n",
    "\n",
    "Backpropagation computes gradients by propagating errors backward from the output layer to the input layer. A key quantity in this process is the *error term* $\\delta^{(l)} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(l)}}$, which represents the gradient of the loss with respect to the pre-activation $\\mathbf{z}^{(l)}$. We define $\\delta^{(l)}$ recursively and use it to compute the gradients for the weights and biases.\n",
    "\n",
    "### Step 1: Output Layer Gradient\n",
    "\n",
    "For the output layer ($l = L$), the error term is:\n",
    "\n",
    "$$\n",
    "\\delta^{(L)} = \\nabla_{\\mathbf{a}^{(L)}} L \\odot \\sigma'^{(L)}(\\mathbf{z}^{(L)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\nabla_{\\mathbf{a}^{(L)}} L = \\frac{\\partial L}{\\partial \\mathbf{a}^{(L)}}$ is the gradient of the loss with respect to the output activation, which depends on the loss function (e.g., for mean squared error $L = \\frac{1}{2} \\|\\mathbf{a}^{(L)} - \\mathbf{t}\\|^2$, it is $\\mathbf{a}^{(L)} - \\mathbf{t}$).\n",
    "- $\\sigma'^{(L)}(\\mathbf{z}^{(L)})$ is the derivative of the activation function at layer $L$, applied element-wise.\n",
    "- $\\odot$ denotes element-wise (Hadamard) multiplication.\n",
    "\n",
    "### Step 2: Gradients for Layer $l$\n",
    "\n",
    "For each layer $l = L, L-1, \\dots, 1$, the gradients with respect to the weights and biases are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(l)}} = \\delta^{(l)} (\\mathbf{a}^{(l-1)})^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}^{(l)}} = \\delta^{(l)}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\delta^{(l)}$ is a column vector whose size matches $\\mathbf{z}^{(l)}$ and $\\mathbf{b}^{(l)}$.\n",
    "- $\\mathbf{a}^{(l-1)}$ is the activation from the previous layer (a column vector), and $(\\mathbf{a}^{(l-1)})^T$ is its transpose (a row vector).\n",
    "- The resulting $\\frac{\\partial L}{\\partial \\mathbf{W}^{(l)}}$ is a matrix with the same dimensions as $\\mathbf{W}^{(l)}$.\n",
    "\n",
    "### Step 3: Propagate Error Backward\n",
    "\n",
    "For hidden layers ($l = L-1, L-2, \\dots, 1$), the error term is computed recursively:\n",
    "\n",
    "$$\n",
    "\\delta^{(l)} = (\\mathbf{W}^{(l+1)})^T \\delta^{(l+1)} \\odot \\sigma'^{(l)}(\\mathbf{z}^{(l)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $(\\mathbf{W}^{(l+1)})^T$ is the transpose of the weight matrix of the next layer.\n",
    "- $\\delta^{(l+1)}$ is the error term from layer $l+1$.\n",
    "- $\\sigma'^{(l)}(\\mathbf{z}^{(l)})$ is the derivative of the activation function at layer $l$.\n",
    "\n",
    "This recursion propagates the error backward through the network, adjusting for the weights and activation functions at each layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Gradient Formulas\n",
    "\n",
    "The mathematical formulas for the gradients in backpropagation are:\n",
    "\n",
    "- **Weights gradient**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(l)}} = \\delta^{(l)} (\\mathbf{a}^{(l-1)})^T\n",
    "$$\n",
    "\n",
    "- **Bias gradient**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}^{(l)}} = \\delta^{(l)}\n",
    "$$\n",
    "\n",
    "where $\\delta^{(l)}$ is defined as:\n",
    "\n",
    "- For the output layer ($l = L$):\n",
    "\n",
    "$$\n",
    "\\delta^{(L)} = \\nabla_{\\mathbf{a}^{(L)}} L \\odot \\sigma'^{(L)}(\\mathbf{z}^{(L)})\n",
    "$$\n",
    "\n",
    "- For hidden layers ($l = L-1, L-2, \\dots, 1$):\n",
    "\n",
    "$$\n",
    "\\delta^{(l)} = (\\mathbf{W}^{(l+1)})^T \\delta^{(l+1)} \\odot \\sigma'^{(l)}(\\mathbf{z}^{(l)})\n",
    "$$\n",
    "\n",
    "These formulas are general and apply to any layer in a feedforward neural network, assuming a differentiable loss function and activation functions. The specific form of $\\nabla_{\\mathbf{a}^{(L)}} L$ depends on the choice of loss (e.g., mean squared error, cross-entropy), and $\\sigma'^{(l)}$ depends on the activation function (e.g., $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$ for sigmoid, or $1$ for linear).\n",
    "\n",
    "---\n",
    "\n",
    "This completes the mathematical formulation of the gradients in backpropagation, presented in a clear and general manner using LaTeX as requested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "* Large n\n",
    "* Sample a *minibatch*\n",
    "  * Unbiased gradient, but large variance\n",
    "* Learning rate\n",
    "* Many epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "* $L_1$-norm (Lasso)\n",
    "* $L_2$-norm (ridge)\n",
    "* Learning rate\n",
    "* Number of epochs and minibatches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frameworks\n",
    "\n",
    "* Meta's `pytorch`\n",
    "  * Literal style\n",
    "  * Easy to use/reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulation Example\n",
    "\n",
    "* Use NN to solve Poisson regression\n",
    "  * A trivial example for demonstration\n",
    "  * No hidden layer\n",
    "  * Keep the essence\n",
    "  \n",
    "* See `data_example/nn_torch.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Network Structures\n",
    "\n",
    "* Time series\n",
    "  * Recurrent NN (RNN)\n",
    "  * Long term and short term memory (LSTM) (See `data_example/nn_LSTM.ipynb`)\n",
    "* Graphics\n",
    "  * Convolutional NN (CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.497974\n",
      "Epoch 100, Loss: 0.467624\n",
      "Epoch 200, Loss: 0.463342\n",
      "Epoch 300, Loss: 0.462317\n",
      "Epoch 400, Loss: 0.461109\n",
      "Epoch 500, Loss: 0.458347\n",
      "Epoch 600, Loss: 0.451663\n",
      "Epoch 700, Loss: 0.435664\n",
      "Epoch 800, Loss: 0.398754\n",
      "Epoch 900, Loss: 0.321377\n",
      "Final Loss: 0.194452\n",
      "\n",
      "Sample Predictions vs Targets:\n",
      "Predicted: 0.1906, Target: 0.0536\n",
      "Predicted: 0.1661, Target: 0.0633\n",
      "Predicted: 0.1653, Target: 0.1440\n",
      "Predicted: 0.1913, Target: -0.0616\n",
      "Predicted: 0.1289, Target: -0.0328\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic time series data (sine wave with noise)\n",
    "def generate_data(n_samples, seq_length):\n",
    "    t = np.linspace(0, 10, n_samples)\n",
    "    data = np.sin(t) + np.random.normal(0, 0.1, n_samples)  # Sine wave + noise\n",
    "    X, y = [], []\n",
    "    for i in range(n_samples - seq_length):\n",
    "        X.append(data[i:i + seq_length])  # Input sequence\n",
    "        y.append(data[i + seq_length])    # Next value to predict\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)\n",
    "\n",
    "# Define RNN class\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"Initialize weights and biases.\"\"\"\n",
    "        # Weight matrices\n",
    "        self.Wxh = np.random.randn(input_size, hidden_size) * 0.01  # Input to hidden\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
    "        self.Why = np.random.randn(hidden_size, output_size) * 0.01  # Hidden to output\n",
    "        # Biases\n",
    "        self.bh = np.zeros((1, hidden_size))  # Hidden bias\n",
    "        self.by = np.zeros((1, output_size))  # Output bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the RNN.\"\"\"\n",
    "        seq_length = X.shape[1]\n",
    "        batch_size = X.shape[0]\n",
    "        hidden_size = self.Whh.shape[0]\n",
    "        \n",
    "        # Initialize hidden state and storage for activations\n",
    "        h = np.zeros((batch_size, hidden_size))\n",
    "        self.hs = [h]  # Store hidden states for backprop\n",
    "        self.xs = [X[:, t, :] for t in range(seq_length)]  # Store inputs\n",
    "        \n",
    "        # Forward pass over time steps\n",
    "        for t in range(seq_length):\n",
    "            h = np.tanh(np.dot(X[:, t, :], self.Wxh) + np.dot(h, self.Whh) + self.bh)\n",
    "            self.hs.append(h)\n",
    "        \n",
    "        # Output layer (at the last time step)\n",
    "        y = np.dot(h, self.Why) + self.by\n",
    "        return y\n",
    "\n",
    "    def backward(self, X, y_true, y_pred, learning_rate):\n",
    "        \"\"\"Backpropagation through time (BPTT).\"\"\"\n",
    "        seq_length = X.shape[1]\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dy = y_pred - y_true  # Gradient of loss (MSE) w.r.t. output\n",
    "        dWhy = np.dot(self.hs[-1].T, dy) / batch_size  # Gradient w.r.t. Why\n",
    "        dby = np.sum(dy, axis=0, keepdims=True) / batch_size  # Gradient w.r.t. by\n",
    "        \n",
    "        # Initialize hidden state gradient\n",
    "        dh_next = np.dot(dy, self.Why.T)  # Gradient from output to last hidden state\n",
    "        \n",
    "        # Backprop through time\n",
    "        for t in range(seq_length - 1, -1, -1):\n",
    "            dh_raw = dh_next * (1 - self.hs[t + 1] ** 2)  # Gradient w.r.t. pre-activation (tanh derivative)\n",
    "            dbh += np.sum(dh_raw, axis=0, keepdims=True) / batch_size\n",
    "            dWxh += np.dot(self.xs[t].T, dh_raw) / batch_size\n",
    "            dWhh += np.dot(self.hs[t].T, dh_raw) / batch_size\n",
    "            dh_next = np.dot(dh_raw, self.Whh.T)  # Propagate gradient to previous time step\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.Wxh -= learning_rate * dWxh\n",
    "        self.Whh -= learning_rate * dWhh\n",
    "        self.Why -= learning_rate * dWhy\n",
    "        self.bh -= learning_rate * dbh\n",
    "        self.by -= learning_rate * dby\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        \"\"\"Train the RNN.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = np.mean((y_pred - y) ** 2)  # Mean Squared Error\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "            self.backward(X, y, y_pred, learning_rate)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate data\n",
    "    n_samples = 1000\n",
    "    seq_length = 10\n",
    "    X, y = generate_data(n_samples, seq_length)\n",
    "    X = X.reshape(-1, seq_length, 1)  # Shape: (samples, timesteps, features)\n",
    "    \n",
    "    # Initialize and train RNN\n",
    "    rnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n",
    "    rnn.train(X, y, epochs=1000, learning_rate=0.01)\n",
    "    \n",
    "    # Test the model\n",
    "    y_pred = rnn.forward(X)\n",
    "    final_loss = np.mean((y_pred - y) ** 2)\n",
    "    print(f\"Final Loss: {final_loss:.6f}\")\n",
    "    \n",
    "    # Optional: Print some predictions\n",
    "    print(\"\\nSample Predictions vs Targets:\")\n",
    "    for i in range(5):\n",
    "        print(f\"Predicted: {y_pred[i][0]:.4f}, Target: {y[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.201597\n",
      "Epoch 100, Loss: 0.014641\n",
      "Epoch 200, Loss: 0.013456\n",
      "Epoch 300, Loss: 0.012553\n",
      "Epoch 400, Loss: 0.012175\n",
      "Epoch 500, Loss: 0.012012\n",
      "Epoch 600, Loss: 0.011856\n",
      "Epoch 700, Loss: 0.011692\n",
      "Epoch 800, Loss: 0.011574\n",
      "Epoch 900, Loss: 0.011523\n",
      "Final Loss: 0.011502\n",
      "\n",
      "Sample Predictions vs Targets:\n",
      "Predicted: 0.1056, Target: 0.0536\n",
      "Predicted: 0.0912, Target: 0.0633\n",
      "Predicted: 0.0830, Target: 0.1440\n",
      "Predicted: 0.0968, Target: -0.0616\n",
      "Predicted: 0.0527, Target: -0.0328\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the RNN class\n",
    "class RNN_torch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_torch, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)  # Initial hidden state\n",
    "        out, _ = self.rnn(x, h0)  # RNN forward pass\n",
    "        out = self.fc(out[:, -1, :])  # Fully connected layer on the last time step\n",
    "        return out\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Initialize the RNN\n",
    "input_size = X.shape[2]\n",
    "hidden_size = 5\n",
    "output_size = 1\n",
    "model = RNN_torch(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the RNN\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Evaluate the final performance\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred = model(X_tensor).numpy()\n",
    "    final_loss = criterion(torch.tensor(final_pred), y_tensor).item()\n",
    "    print(f\"Final Loss: {final_loss:.6f}\")\n",
    "\n",
    "# Optional: Print some predictions\n",
    "print(\"\\nSample Predictions vs Targets:\")\n",
    "for i in range(5):\n",
    "    print(f\"Predicted: {final_pred[i][0]:.4f}, Target: {y[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Theory is Incomplete\n",
    "\n",
    "* Theoretical understanding is an ongoing endeavor.\n",
    "* Hornik, Stinchcombe, and White (1989):\n",
    "  * A single hidden layer neural network, given enough many nodes, is a *universal approximator* for any measurable function.\n",
    "* Deep learning: engineering breakthrough\n",
    "* Big data available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- ## Reinforcement Learning\n",
    "\n",
    "* Policy function $d(x_t; \\theta)$, where $\\theta$ is the parameter\n",
    "* Response $y_t$, with reward $r( d (x_t; \\theta), y_t)$\n",
    "* Optimal invariant parameter\n",
    "$$\n",
    "\\theta^* = \\arg \\max_{\\theta} \\sum_{t=1}^T r( d(x_t; \\theta), y_t)\n",
    "$$\n",
    "\n",
    "* Regret: \n",
    "\n",
    "$$\n",
    " \\sum_{t=1}^T [ r( d(x_t; \\theta^*), y_t) - r( d(x_t; \\theta_t), y_t)] \n",
    "$$ -->"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "rise": {
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
