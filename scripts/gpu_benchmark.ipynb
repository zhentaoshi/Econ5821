{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Benchmark Testing\n",
    "\n",
    "This notebook tests GPU performance using CuPy with matrix operations and random number generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "except ImportError:\n",
    "    print(\"CuPy is not installed. Run: pip install cupy-cuda12x\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "CUDARuntimeError",
     "evalue": "cudaErrorNoDevice: no CUDA-capable device is detected",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCUDARuntimeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get GPU info\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m n_gpu = \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mruntime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetDeviceCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_gpu < \u001b[32m1\u001b[39m:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError: No CUDA device detected\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends/cuda/api/runtime.pyx:420\u001b[39m, in \u001b[36mcupy_backends.cuda.api.runtime.getDeviceCount\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends/cuda/api/runtime.pyx:423\u001b[39m, in \u001b[36mcupy_backends.cuda.api.runtime.getDeviceCount\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends/cuda/api/runtime.pyx:146\u001b[39m, in \u001b[36mcupy_backends.cuda.api.runtime.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCUDARuntimeError\u001b[39m: cudaErrorNoDevice: no CUDA-capable device is detected"
     ]
    }
   ],
   "source": [
    "# Get GPU info\n",
    "n_gpu = cp.cuda.runtime.getDeviceCount()\n",
    "if n_gpu < 1:\n",
    "    print(\"Error: No CUDA device detected\")\n",
    "else:\n",
    "    props = cp.cuda.runtime.getDeviceProperties(0)\n",
    "    gpu_name = props[\"name\"].decode(\"utf-8\")\n",
    "    multi_processor_count = props[\"multiProcessorCount\"]\n",
    "    max_threads_per_multiprocessor = props.get(\"maxThreadsPerMultiprocessor\", 2048)\n",
    "    \n",
    "    print(f\"GPU Device: {gpu_name}\")\n",
    "    print(f\"CUDA Version: {cp.cuda.runtime.runtimeGetVersion()}\")\n",
    "    print(f\"CuPy Version: {cp.__version__}\")\n",
    "    print(f\"Multiprocessors: {multi_processor_count}\")\n",
    "    print(f\"Max Threads per MP: {max_threads_per_multiprocessor}\")\n",
    "    print(f\"Total CUDA Cores: ~{multi_processor_count * 128}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matrix_mult(size):\n",
    "    \"\"\"Benchmark matrix multiplication.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Matrix Multiplication: {size}x{size}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # CPU\n",
    "    A_cpu = np.random.rand(size, size)\n",
    "    B_cpu = np.random.rand(size, size)\n",
    "    t0 = time.time()\n",
    "    C_cpu = A_cpu @ B_cpu\n",
    "    cpu_time = time.time() - t0\n",
    "    print(f\"CPU:  {cpu_time:.4f} seconds\")\n",
    "\n",
    "    # GPU\n",
    "    A_gpu = cp.array(A_cpu)\n",
    "    B_gpu = cp.array(B_cpu)\n",
    "    t0 = time.time()\n",
    "    C_gpu = A_gpu @ B_gpu\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    gpu_time = time.time() - t0\n",
    "    print(f\"GPU:  {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    \n",
    "    return cpu_time, gpu_time\n",
    "\n",
    "\n",
    "def benchmark_random_stats(samples, size):\n",
    "    \"\"\"Benchmark random number generation and statistics.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Random Generation & Stats: {samples:,} samples of {size:,} elements\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # CPU\n",
    "    t0 = time.time()\n",
    "    x_cpu = np.random.randn(samples, size)\n",
    "    mean_cpu = np.mean(x_cpu, axis=1)\n",
    "    std_cpu = np.std(x_cpu, axis=1)\n",
    "    cpu_time = time.time() - t0\n",
    "    print(f\"CPU:  {cpu_time:.4f} seconds\")\n",
    "\n",
    "    # GPU\n",
    "    t0 = time.time()\n",
    "    x_gpu = cp.random.randn(samples, size)\n",
    "    mean_gpu = cp.mean(x_gpu, axis=1)\n",
    "    std_gpu = cp.std(x_gpu, axis=1)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    gpu_time = time.time() - t0\n",
    "    print(f\"GPU:  {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    \n",
    "    return cpu_time, gpu_time\n",
    "\n",
    "\n",
    "def benchmark_elementwise(size):\n",
    "    \"\"\"Benchmark element-wise operations.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Element-wise Operations: {size:,} elements\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # CPU\n",
    "    x_cpu = np.random.rand(size)\n",
    "    y_cpu = np.random.rand(size)\n",
    "    t0 = time.time()\n",
    "    z_cpu = 2.0 * x_cpu**2 + 3.0 * y_cpu**3 + np.sin(x_cpu) * np.cos(y_cpu)\n",
    "    cpu_time = time.time() - t0\n",
    "    print(f\"CPU:  {cpu_time:.4f} seconds\")\n",
    "\n",
    "    # GPU\n",
    "    x_gpu = cp.array(x_cpu)\n",
    "    y_gpu = cp.array(y_cpu)\n",
    "    t0 = time.time()\n",
    "    z_gpu = 2.0 * x_gpu**2 + 3.0 * y_gpu**3 + cp.sin(x_gpu) * cp.cos(y_gpu)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    gpu_time = time.time() - t0\n",
    "    print(f\"GPU:  {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    \n",
    "    return cpu_time, gpu_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Element-wise Operations: 10,000,000 elements\n",
      "============================================================\n",
      "CPU:  0.4419 seconds\n"
     ]
    },
    {
     "ename": "CUDARuntimeError",
     "evalue": "cudaErrorNoDevice: no CUDA-capable device is detected",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCUDARuntimeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m results = {}\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Element-wise benchmark\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m cpu_ew, gpu_ew = \u001b[43mbenchmark_elementwise\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10_000_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mElement-wise\u001b[39m\u001b[33m'\u001b[39m] = {\u001b[33m'\u001b[39m\u001b[33mCPU\u001b[39m\u001b[33m'\u001b[39m: cpu_ew, \u001b[33m'\u001b[39m\u001b[33mGPU\u001b[39m\u001b[33m'\u001b[39m: gpu_ew, \u001b[33m'\u001b[39m\u001b[33mSpeedup\u001b[39m\u001b[33m'\u001b[39m: cpu_ew/gpu_ew}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mbenchmark_elementwise\u001b[39m\u001b[34m(size)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCPU:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# GPU\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m x_gpu = \u001b[43mcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m y_gpu = cp.array(y_cpu)\n\u001b[32m     72\u001b[39m t0 = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/cupy/_creation/from_data.py:53\u001b[39m, in \u001b[36marray\u001b[39m\u001b[34m(obj, dtype, copy, order, subok, ndmin, blocking)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34marray\u001b[39m(obj, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, order=\u001b[33m'\u001b[39m\u001b[33mK\u001b[39m\u001b[33m'\u001b[39m, subok=\u001b[38;5;28;01mFalse\u001b[39;00m, ndmin=\u001b[32m0\u001b[39m, *,\n\u001b[32m      8\u001b[39m           blocking=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Creates an array on the current device.\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[33;03m    This function currently does not support the ``subok`` option.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \n\u001b[32m     52\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/_core/core.pyx:2502\u001b[39m, in \u001b[36mcupy._core.core.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/_core/core.pyx:2529\u001b[39m, in \u001b[36mcupy._core.core.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/_core/core.pyx:2740\u001b[39m, in \u001b[36mcupy._core.core._array_default\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/_core/core.pyx:167\u001b[39m, in \u001b[36mcupy._core.core.ndarray.__new__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/_core/core.pyx:254\u001b[39m, in \u001b[36mcupy._core.core._ndarray_base._init\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/cuda/memory.pyx:875\u001b[39m, in \u001b[36mcupy.cuda.memory.alloc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/cuda/memory.pyx:1579\u001b[39m, in \u001b[36mcupy.cuda.memory.MemoryPool.malloc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/cuda/memory.pyx:1599\u001b[39m, in \u001b[36mcupy.cuda.memory.MemoryPool.malloc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy/cuda/device.pyx:40\u001b[39m, in \u001b[36mcupy.cuda.device.get_device_id\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends/cuda/api/runtime.pyx:202\u001b[39m, in \u001b[36mcupy_backends.cuda.api.runtime.getDevice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mcupy_backends/cuda/api/runtime.pyx:146\u001b[39m, in \u001b[36mcupy_backends.cuda.api.runtime.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCUDARuntimeError\u001b[39m: cudaErrorNoDevice: no CUDA-capable device is detected"
     ]
    }
   ],
   "source": [
    "# Storage for results\n",
    "results = {}\n",
    "\n",
    "# Element-wise benchmark\n",
    "cpu_ew, gpu_ew = benchmark_elementwise(10_000_000)\n",
    "results['Element-wise'] = {'CPU': cpu_ew, 'GPU': gpu_ew, 'Speedup': cpu_ew/gpu_ew}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random generation & statistics benchmark\n",
    "cpu_rs, gpu_rs = benchmark_random_stats(100_000, 1000)\n",
    "results['Random Stats'] = {'CPU': cpu_rs, 'GPU': gpu_rs, 'Speedup': cpu_rs/gpu_rs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication benchmark\n",
    "cpu_mm, gpu_mm = benchmark_matrix_mult(2048)\n",
    "results['Matrix Mult'] = {'CPU': cpu_mm, 'GPU': gpu_mm, 'Speedup': cpu_mm/gpu_mm}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PERFORMANCE COMPARISON SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Benchmark':<20} {'CPU Time (s)':<15} {'GPU Time (s)':<15} {'Speedup':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for name, data in results.items():\n",
    "    print(f\"{name:<20} {data['CPU']:<15.4f} {data['GPU']:<15.4f} {data['Speedup']:<10.2f}x\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Calculate average speedup\n",
    "avg_speedup = np.mean([data['Speedup'] for data in results.values()])\n",
    "print(f\"\\nAverage Speedup: {avg_speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "- **GPU excels at parallel random number generation and statistics** - significant speedup for Monte Carlo-type simulations\n",
    "- **Element-wise operations on smaller arrays** - data transfer overhead can negate GPU benefits\n",
    "- **Matrix multiplication** - requires larger matrices to fully utilize GPU parallelism and overcome transfer costs\n",
    "\n",
    "### When to Use GPU:\n",
    "1. Large-scale Monte Carlo simulations (like the Poisson CI coverage estimation in `parallel_gpu.py`)\n",
    "2. Batch operations on large datasets (100K+ samples)\n",
    "3. Compute-intensive parallel tasks with minimal CPU-GPU data transfer\n",
    "\n",
    "### When CPU May Be Better:\n",
    "1. Small to medium problem sizes with significant data transfer overhead\n",
    "2. Serial operations with minimal parallelizability\n",
    "3. Quick prototyping where GPU allocation overhead isn't justified"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
