{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Integration\n",
    "\n",
    "### Zhentao Shi\n",
    "\n",
    "<img src=\"graph/damaoshan.jpeg\" width=\"1000\">\n",
    "\n",
    "<!-- code is tested on SCRP -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Stochastic integration\n",
    "* Markov Chain Monte Carlo (MCMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Numerical Methods\n",
    "\n",
    "* Numerical differentiation and integration\n",
    "\n",
    "* Use case: To find the optimum for the objective function $f:R^K \\mapsto R$\n",
    "by Newton's method, \n",
    "  * $K$-dimensional gradient   \n",
    "  * $K\\times K$-dimensional Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Programming up the gradient and the Hessian manually is a time-consuming and error-prone job\n",
    "* Whenever we change the objective function, we have to redo the gradient and Hessian\n",
    "\n",
    "* More efficient to use numerical differentiation instead of the analytical expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The partial derivative of a multivariate function\n",
    "$f:R^K \\mapsto R$ at a point $x_0 \\in R^K$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x_k}\\bigg|_{x=x_0}=\\lim_{\\epsilon \\to 0}\n",
    "\\frac{f(x_0+\\epsilon \\cdot e_k) - f(x_0 - \\epsilon \\cdot e_k)}{2\\epsilon},\n",
    "$$\n",
    "\n",
    "where $e_k = (0,\\ldots,0,1,0,\\ldots,0)$ is the identifier of the $k$-th coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Numerical execution in a computer follows the basic definition to evaluate\n",
    "$f(x_0\\pm\\epsilon \\cdot e_k))$ with a small\n",
    "$\\epsilon$. \n",
    "\n",
    "* How small is small? Usually we try a sequence of $\\epsilon$'s until\n",
    "the numerical derivative is stable. \n",
    "\n",
    "* There are also more sophisticated algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Caution\n",
    "\n",
    "* Numerical methods are not panacea\n",
    "* Not all functions are differentiable or integrable.\n",
    "* Before turning to numerical methods, it is always imperative to try to understand the behavior of the function at the first place.\n",
    "* AI tools can be very helpful in these math calculation and coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Integration\n",
    "\n",
    "* An alternative to analytic/numerical integration is the stochastic methods.\n",
    "* The underlying principle of stochastic integration is the law of large numbers.\n",
    "\n",
    "* Let  $\\int h(x) d F(x)$ be an integral where $F(x)$ is a probability distribution.\n",
    "* Approximate the integral by\n",
    "$\\int h(x) d F(x) \\approx S^{-1} \\sum_{s=1}^S h(x_s)$, where $x_s$ is randomly\n",
    "generated from $F(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* When $S$ is large, a law of large numbers gives\n",
    "\n",
    "$$\n",
    "S^{-1} \\sum_{s=1}^S h(x_s) \\stackrel{\\mathrm{p}}{\\to} E[h(x)] = \\int h(x) d F(x).\n",
    "$$\n",
    "\n",
    "* If the integration is carried out not in the entire support of $F(x)$ but on a subset $A$, then\n",
    "\n",
    "$$\n",
    "\\int_A h(x) d F(x) \\approx S^{-1} \\sum_{s=1}^S h(x_s) \\cdot 1\\{x_s \\in A\\},\n",
    "$$\n",
    "\n",
    "where $1\\{\\cdot\\}$ is the indicator function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In theory, use an $S$ as large as possible.\n",
    "* In reality, constrained by the computer's memory and computing time.\n",
    "* No clear guidance of the size of $S$ in practice. \n",
    "* Preliminary experiment can help decide an $S$ that produces stable results.\n",
    "\n",
    "* Stochastic integration is popular in econometrics and statistics, thanks to its convenience in execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example**: Use simulated `Probit` maximum likelihood to estimate the parameters of a binary choice model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "beta_true = [0.5, 0.5]  # True coefficients\n",
    "n = 1000           # Number of observations\n",
    "k = len(beta_true)      # Number of coefficients\n",
    "\n",
    "# Generate data\n",
    "X = np.random.rand(n, k) \n",
    "X[:, 0] = 1  # Intercept\n",
    "\n",
    "error = np.random.normal(0, 1, size=n)\n",
    "y_latent = np.dot(X, beta_true) + error\n",
    "\n",
    "y = (y_latent > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nlopt\n",
    "\n",
    "# Define the simulated likelihood function\n",
    "def sim_likelihood(params, n_sims=500):\n",
    "    \"\"\"\n",
    "    Simulated likelihood function for the Probit model.\n",
    "    \"\"\"\n",
    "    beta = params\n",
    "    # Simulate the latent variable\n",
    "    error_sim = np.random.normal(0, 1, size=(n, n_sims))\n",
    "    y_latent_sim = np.tile( (X @ beta).reshape(-1, 1), n_sims) + error_sim\n",
    "    # Calculate the probability of observed y\n",
    "    prob = np.mean(y_latent_sim > 0, axis=1)\n",
    "    # Likelihood contribution\n",
    "    ll = np.sum(y * np.log(prob) + (1 - y) * np.log(1 - prob))\n",
    "    return -ll  # Minimize the negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate the parameters using SML\n",
    "# Initial guess for parameters\n",
    "beta_guess = [0.2, 0.2]\n",
    "\n",
    "# Define the objective function for nlopt\n",
    "def objective_function(params, grad):\n",
    "    return sim_likelihood(params)\n",
    "\n",
    "# Set up the optimizer\n",
    "opt = nlopt.opt(nlopt.LN_NELDERMEAD, len(beta_guess))\n",
    "opt.set_min_objective(objective_function)\n",
    "opt.set_xtol_rel(1e-6)\n",
    "\n",
    "# Run the optimization\n",
    "beta_opt = opt.optimize(beta_guess)\n",
    "minf = opt.last_optimum_value()\n",
    "\n",
    "# Print the results\n",
    "print(\"True parameters:\", beta_true)\n",
    "print(\"Estimated parameters:\", beta_opt)\n",
    "print(\"Value of the criterion function:\", minf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Variable Generation\n",
    "\n",
    "* **If the CDF $F(X)$ is known**, we can generate random variables that follow such a distribution.\n",
    "  * Draw $U$ from  $\\mathrm{Uniform}(0,1)$\n",
    "  * Compute $X = F^{-1}(U)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov Chain Monte Carlo\n",
    "\n",
    "* **If the pdf $f(X)$ is known**, we can generate a sample by *importance sampling*\n",
    "  * [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)\n",
    " (MH algorithm) is such a method.\n",
    "  * MH is one of the [Markov Chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Metropolis-Hastings Algorithm\n",
    "\n",
    "* Theory of the MH requires long derivation\n",
    "* Implementation is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Use MH to generate a sample of normally distributed observations with\n",
    "$\\mu = 1$ and $\\sigma = 0.5$.\n",
    "\n",
    "* In the function `metrop`, we provide the logarithm of the density of\n",
    "\n",
    "$$\n",
    "\\log f(x) = -\\frac{1}{2} \\log (2\\pi) - \\log \\sigma - \\frac{1}{2\\sigma^2} (x-\\mu)^2\n",
    "$$\n",
    "\n",
    "  * The first term can be omitted as it is irrelevant to the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "\n",
    "# the target distribution\n",
    "def h(x, mu=1, sd=0.5):\n",
    "    return -np.log(sd) - (x - mu)**2 / (2 * sd**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# a self-defined metropolis algorithm\n",
    "\n",
    "def metropolis(h, initial, n_samples, n_spac):\n",
    "# h: log density function. It's a function. \n",
    "# initial: initial value\n",
    "# n_samples: total number of samples to generate\n",
    "# n_spac: number of samples to generate before saving one\n",
    "    \n",
    "    total_iter = n_samples * n_spac\n",
    "    chain = []\n",
    "    current = initial\n",
    "    current_h = h(current)\n",
    "    for i in range(total_iter):\n",
    "        # proposal = current + np.random.normal(0, proposal_sd) # normal proposal distribution\n",
    "        proposal = current + np.random.rand()-.5 # uniform proposal distribution\n",
    "        # the proposal distribution is symmetric. It determines the acceptance probability\n",
    "        proposal_h = h(proposal)\n",
    "        if np.log(np.random.rand()) < (proposal_h - current_h):\n",
    "            current = proposal\n",
    "            current_h = proposal_h\n",
    "        if (i + 1) % n_spac == 0:\n",
    "            chain.append(current)\n",
    "    return np.array(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(8, 12))\n",
    "\n",
    "# 1) Time series with flat steps: nbatch=100, nspac=1\n",
    "chain1 = metropolis(h, initial=0, n_samples=100, n_spac=1)\n",
    "axes[0].plot(chain1, linestyle='-', marker='o')\n",
    "axes[0].set_title(\"Time series with flat steps (nspac=1)\")\n",
    "\n",
    "# 2) Time series with reduced serial correlation: nbatch=100, nspac=10\n",
    "chain2 = metropolis(h, initial=0, n_samples=100, n_spac=10)\n",
    "axes[1].plot(chain2, linestyle='-', marker='o')\n",
    "axes[1].set_title(\"Time series looks like white noise (nspac=10)\")\n",
    "\n",
    "# 3) Density estimate versus true density: nbatch=10000, nspac=10\n",
    "chain3 = metropolis(h, initial=0, n_samples=10000, n_spac=10)\n",
    "density = gaussian_kde(chain3)\n",
    "xbase = np.linspace(-2, 2, 400)\n",
    "axes[2].plot(xbase, density(xbase), lw=2, label=\"Kernel Density\")\n",
    "axes[2].plot(xbase, norm.pdf(xbase, loc=1, scale=0.5), color='red', label=\"True Density\")\n",
    "axes[2].set_title(\"Kernel density of simulated observations\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Outcomes\n",
    "\n",
    "* The first panel is a time series where\n",
    "the marginal distribution of each observations follows $N(1,0.5^2)$. \n",
    "  * Time dependence is visible\n",
    "  * flat regions are observed when the Markov chain rejects a new proposal\n",
    "so the value does not update over two periods. \n",
    "\n",
    "* The middle panel collects the time series every 10 observations on the Markov chain\n",
    "  * serial correlation is weakened\n",
    "  * No flat region is observed\n",
    " \n",
    "  \n",
    "* The third panel compares     \n",
    "  * kernel density of the simulated observations (black curve) \n",
    "  * density function of $N(1,0.5^2)$ (red curve).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Inference\n",
    "\n",
    "\n",
    "* Bayesian framework offers a coherent and natural language for statistical decision. \n",
    "* Bayesian views both the data $\\mathbf{X}_{n}$ and the\n",
    "parameter $\\theta$ as random variables\n",
    "* Before observeing the data, researcher holds a *prior distribution* $\\pi$ about $\\theta$\n",
    "* After observing the data, researcher updates the prior distribution to a *posterior distribution* $p(\\theta|\\mathbf{X}_{n})$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "* Let $f(\\mathbf{X}_{n}|\\theta)$ be the likelihood\n",
    "* Let $\\pi$ be the prior\n",
    "\n",
    "* The celebrated Bayes Theorem is\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathbf{X}_{n})\\propto f(\\mathbf{X}_{n}|\\theta)\\pi(\\theta)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classical Analytical Example \n",
    "\n",
    "* $\\mathbf{X}_{n}=(X_{1},\\ldots,X_{n})$ is an iid sample drawn from a normal distribution with unknown $\\theta$ and known $\\sigma$\n",
    "* If a researcher's prior distribution\n",
    "$\\theta\\sim N(\\theta_{0},\\sigma_{0}^{2})$, her posterior distribution\n",
    "is, by some routine calculation, also a normal distribution\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathbf{x}_{n})\\sim N\\left(\\tilde{\\theta},\\tilde{\\sigma}^{2}\\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "$\\tilde{\\theta}=\\frac{\\sigma_{0}^{2}}{n\\sigma^{2}+\\sigma_{0}^{2}}\\theta_{0}+\\frac{n\\sigma^{2}}{n\\sigma^{2}+\\sigma_{0}^{2}}\\bar{x}$\n",
    "and\n",
    "$\\tilde{\\sigma}^{2}=\\frac{\\sigma_{0}^{2}\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Credible Set\n",
    "\n",
    "$$\n",
    "\\left(\\tilde{\\theta}-z_{1-\\alpha/2}\\cdot\\tilde{\\sigma},\\ \\tilde{\\theta}+z_{1-\\alpha/2}\\cdot\\tilde{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "* Posterior distribution depends on $\\theta_{0}$ and $\\sigma_{0}^{2}$\n",
    "from the prior. \n",
    "* When the sample size is sufficiently, the data overwhelms the prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Frequentist Confidence Internval\n",
    "\n",
    "\n",
    "* $\\hat{\\theta}=\\bar{x}\\sim N(\\theta,\\sigma^{2}/n)$. \n",
    "\n",
    "* Confidence interval is\n",
    "\n",
    "$$\n",
    "\\left(\\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n},\\ \\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparison\n",
    "\n",
    "* Bayesian produces a posterior distribution\n",
    "  * The posterior distribution implies point estimates and credible set\n",
    "  * Data are fixed (invariant)\n",
    "  * A prior distribution is needed\n",
    "\n",
    "* Frequestist produces a point estimator\n",
    "  * Before data are observed, the point estimator is random\n",
    "  * Inference is imply by the point estimator **before observation**\n",
    "  * Only data are realized, the point estimate is a fixed number\n",
    "  * No prior distribution is needed\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The nature generates data\n",
    "* The researcher correctly specifies the normal model\n",
    "* Given the data, she infers $\\theta$ according to the posterior distribution\n",
    "\n",
    "### Code Example\n",
    "\n",
    "* Conduct inference about the mean parameter of a normal with unit variance\n",
    "* Prior distribution $\\mu \\sim Beta(2,2)$\n",
    "* [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) is flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "n = 2000\n",
    "\n",
    "# generate n random numbers from a normal distribution\n",
    "x = np.random.normal(size=n) + 0.9\n",
    "\n",
    "def loglik(theta):\n",
    "    # Computes the sum of log densities of x ~ N(theta, 1)\n",
    "    return np.sum(norm.logpdf(x, loc=theta, scale=1))\n",
    "\n",
    "def posterior(theta):\n",
    "    # Adds the log density of a Beta(2,2) prior to the log-likelihood\n",
    "    return loglik(theta) + beta.logpdf(theta, a=2, b=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nbatch = 10000\n",
    "out = metropolis(posterior, initial=0.1, n_samples=nbatch, n_spac=10)\n",
    "burn_in = round(nbatch / 10)\n",
    "out = out[burn_in:]  # remove the burn-in period\n",
    "\n",
    "# Plot the kernel density of the posterior draws\n",
    "density_est = gaussian_kde(out)\n",
    "x_grid = np.linspace(out.min(), out.max(), 200)\n",
    "\n",
    "# Print the 95% quantile interval\n",
    "quat = np.quantile(out, [0.025, 0.975])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(quat)\n",
    "\n",
    "# plot the two values in quat as vertical lines\n",
    "plt.figure()\n",
    "plt.plot(x_grid, density_est(x_grid))\n",
    "\n",
    "plt.axvline(quat[0], color='red', linestyle='--')\n",
    "plt.axvline(quat[1], color='red', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe95f1d",
   "metadata": {},
   "source": [
    "### Finance Application with Real Data: Bayesian Credit-Risk Level\n",
    "\n",
    "* **Industry context**: A bank tracks U.S. credit-card delinquency risk for stress testing and provisioning.\n",
    "* **Real data source**: FRED series `DRCCLACBS` (Delinquency Rate on Credit Card Loans, All Commercial Banks, Percent).\n",
    "* We estimate the underlying delinquency level $\\theta$ (in percentage points) for the recent regime.\n",
    "\n",
    "Model:\n",
    "\n",
    "$$\n",
    "y_t \\mid \\theta \\sim N(\\theta, \\sigma^2), \\quad t=1,\\ldots,n\n",
    "$$\n",
    "\n",
    "with prior\n",
    "\n",
    "$$\n",
    "\\theta \\sim N(\\theta_0, \\tau_0^2).\n",
    "$$\n",
    "\n",
    "Posterior (normal-normal conjugacy):\n",
    "\n",
    "$$\n",
    "\\theta \\mid \\mathbf{y} \\sim N(\\tilde{\\theta}, \\tilde{\\tau}^2),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\tilde{\\tau}^2 = \\left(\\frac{n}{\\sigma^2}+\\frac{1}{\\tau_0^2}\\right)^{-1},\n",
    "\\quad\n",
    "\\tilde{\\theta} = \\tilde{\\tau}^2\\left(\\frac{n\\bar{y}}{\\sigma^2}+\\frac{\\theta_0}{\\tau_0^2}\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "# Real data from FRED\n",
    "series_id = 'DRCCLACBS'  # Delinquency Rate on Credit Card Loans, %\n",
    "raw = web.DataReader(series_id, 'fred', '2000-01-01', '2025-12-31').dropna()\n",
    "\n",
    "# Focus on recent regime for risk monitoring\n",
    "y = raw.loc['2016-01-01':, series_id].astype(float)\n",
    "\n",
    "# Likelihood with plug-in sigma (sample std of observed rates)\n",
    "n = len(y)\n",
    "ybar = y.mean()\n",
    "sigma = y.std(ddof=1)\n",
    "sigma2 = sigma**2\n",
    "\n",
    "# Prior belief from historical risk management experience\n",
    "theta0 = 3.0   # prior mean delinquency rate in %\n",
    "tau0 = 1.0     # prior std in percentage points\n",
    "tau02 = tau0**2\n",
    "\n",
    "# Posterior update\n",
    "tau_post2 = 1.0 / (n / sigma2 + 1.0 / tau02)\n",
    "theta_post = tau_post2 * (n * ybar / sigma2 + theta0 / tau02)\n",
    "tau_post = np.sqrt(tau_post2)\n",
    "\n",
    "ci = norm.ppf([0.025, 0.975], loc=theta_post, scale=tau_post)\n",
    "\n",
    "print(f\"FRED series: {series_id}\")\n",
    "print(f\"Sample window: {y.index.min().date()} to {y.index.max().date()} ({n} quarters)\")\n",
    "print(f\"Sample mean delinquency rate: {ybar:.3f}%\")\n",
    "print(f\"Posterior mean delinquency rate: {theta_post:.3f}%\")\n",
    "print(f\"95% Bayesian credible interval: [{ci[0]:.3f}%, {ci[1]:.3f}%]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior vs posterior density for delinquency level (percentage points)\n",
    "grid = np.linspace(max(0.1, theta_post - 4 * tau0), theta_post + 4 * tau0, 400)\n",
    "prior_pdf = norm.pdf(grid, loc=theta0, scale=tau0)\n",
    "post_pdf = norm.pdf(grid, loc=theta_post, scale=tau_post)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(grid, prior_pdf, label=r'Prior $N(\\theta_0, \\tau_0^2)$', linewidth=2)\n",
    "plt.plot(grid, post_pdf, label=r'Posterior $N(\\tilde{\\theta}, \\tilde{\\tau}^2)$', linewidth=2)\n",
    "plt.axvline(ybar, color='black', linestyle='--', label='Sample mean')\n",
    "plt.title('Bayesian Update: U.S. Credit-Card Delinquency Level')\n",
    "plt.xlabel('Delinquency rate (%)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(y.index, y.values, marker='o', linewidth=1.5)\n",
    "plt.axhline(theta_post, color='red', linestyle='--', label='Posterior mean')\n",
    "plt.title('FRED DRCCLACBS (Quarterly)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Delinquency rate (%)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a5e42",
   "metadata": {},
   "source": [
    "### Posterior Expected Loss Distribution\n",
    "\n",
    "Suppose a credit-card portfolio has exposure-at-default (EAD) of $50 million and loss-given-default (LGD) of 60%.\n",
    "\n",
    "Using delinquency rate as a portfolio risk proxy,\n",
    "\n",
    "$$\n",
    "\\text{Expected Loss} = \\text{EAD} \\times \\text{LGD} \\times \\frac{\\theta}{100}.\n",
    "$$\n",
    "\n",
    "Since $\\theta$ has a posterior distribution, expected loss also has a posterior distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf15a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ead = 50_000_000\n",
    "lgd = 0.60\n",
    "\n",
    "# Posterior simulation\n",
    "np.random.seed(5821)\n",
    "theta_draws = np.random.normal(theta_post, tau_post, size=10000)\n",
    "theta_draws = np.clip(theta_draws, 0.0, None)  # keep rates nonnegative\n",
    "loss_draws = ead * lgd * (theta_draws / 100.0)\n",
    "\n",
    "loss_mean = loss_draws.mean()\n",
    "loss_q05, loss_q95 = np.quantile(loss_draws, [0.05, 0.95])\n",
    "\n",
    "print(f\"Posterior mean expected loss: ${loss_mean:,.0f}\")\n",
    "print(f\"90% posterior interval: [${loss_q05:,.0f}, ${loss_q95:,.0f}]\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(loss_draws / 1e6, bins=40, color='steelblue', edgecolor='white')\n",
    "plt.axvline(loss_mean / 1e6, color='red', linestyle='--', label='Posterior mean')\n",
    "plt.title('Posterior Distribution of Expected Credit Loss')\n",
    "plt.xlabel('Loss (million USD)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ac342",
   "metadata": {},
   "source": [
    "*Interpretation*: This real-data Bayesian workflow turns public macro-financial credit indicators into a full uncertainty distribution for portfolio loss, which is directly usable for provisioning, risk limits, and scenario communication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08270cd9",
   "metadata": {},
   "source": [
    "### Extension: Non-Normal Prior + MCMC Posterior\n",
    "\n",
    "Now keep the same real-data likelihood and use a **non-normal prior** for the delinquency level:\n",
    "\n",
    "$$\n",
    "\\theta \\sim \\text{Gamma}(a,b), \\quad \\theta>0.\n",
    "$$\n",
    "\n",
    "We directly invoke `emcee` and use a **Gaussian Metropolis-Hastings move** (`emcee.moves.GaussianMove`) to sample from the posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da425b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emcee\n",
    "from scipy.stats import gamma, gaussian_kde\n",
    "\n",
    "# Reuse y and sigma from the real-data section above\n",
    "# Non-normal prior: Gamma(shape=a_prior, scale=b_prior)\n",
    "a_prior = 3.0\n",
    "b_prior = 1.0  # prior mean = a_prior * b_prior = 3.0\n",
    "\n",
    "# log posterior for emcee (input theta is a 1-d vector)\n",
    "def logpost_gamma_prior(theta_vec):\n",
    "    theta = float(theta_vec[0])\n",
    "    if theta <= 0:\n",
    "        return -np.inf\n",
    "    ll = np.sum(norm.logpdf(y.values, loc=theta, scale=sigma)) # AI's writing-style is not good\n",
    "    lp = gamma.logpdf(theta, a=a_prior, scale=b_prior)\n",
    "    return ll + lp # \"ll\" is log-likelihood, \"lp\" is log-prior \n",
    "\n",
    "print(f\"Prior: Gamma(shape={a_prior:.1f}, scale={b_prior:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(5821)\n",
    "\n",
    "# emcee setup\n",
    "ndim = 1\n",
    "nwalkers = 32\n",
    "proposal_sd = 0.08\n",
    "\n",
    "init_center = max(0.5, float(y.mean()))\n",
    "p0 = init_center + 0.05 * np.random.randn(nwalkers, ndim)\n",
    "p0 = np.clip(p0, 1e-3, None)\n",
    "\n",
    "# GaussianMove is a Metropolis-Hastings random-walk proposal\n",
    "mh_move = emcee.moves.GaussianMove(cov=proposal_sd**2)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, logpost_gamma_prior, moves=mh_move)\n",
    "\n",
    "# burn-in + production\n",
    "state = sampler.run_mcmc(p0, 3000, progress=False)\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(state, 8000, progress=False)\n",
    "\n",
    "draws = sampler.get_chain(flat=True)[:, 0]\n",
    "accept_rate = np.mean(sampler.acceptance_fraction)\n",
    "\n",
    "mcmc_mean = draws.mean()\n",
    "mcmc_ci = np.quantile(draws, [0.025, 0.975])\n",
    "\n",
    "print(f\"Mean acceptance fraction (emcee): {accept_rate:.3f}\")\n",
    "print(f\"Posterior mean (MCMC): {mcmc_mean:.3f}%\")\n",
    "print(f\"95% credible interval (MCMC): [{mcmc_ci[0]:.3f}%, {mcmc_ci[1]:.3f}%]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcc7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics + posterior visualization\n",
    "chain_full = sampler.get_chain()[:, :, 0]  # shape: (n_steps, n_walkers)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Trace of one walker\n",
    "axes[0].plot(chain_full[:, 0], linewidth=0.8)\n",
    "axes[0].set_title('Trace Plot (Walker 1)')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('theta (%)')\n",
    "# Prior vs posterior: histogram of MCMC draws + prior Gamma pdf\n",
    "grid = np.linspace(max(0.01, draws.min() - 0.3), draws.max() + 0.3, 400)\n",
    "prior_pdf = gamma.pdf(grid, a=a_prior, scale=b_prior)\n",
    "post_kde = gaussian_kde(draws)\n",
    "\n",
    "axes[1].hist(draws, bins=40, density=True, color='steelblue', edgecolor='white', alpha=0.7, label='Posterior draws')\n",
    "# axes[1].plot(grid, prior_pdf, label='Prior Gamma', linewidth=2, linestyle='--', color='tab:orange')\n",
    "axes[1].axvline(mcmc_mean, color='red', linestyle='--', label='Posterior mean')\n",
    "\n",
    "axes[1].set_title('Prior vs Posterior (emcee Gaussian MH move)')\n",
    "axes[1].set_xlabel('theta (%)')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implied expected-loss distribution under emcee posterior draws\n",
    "theta_draws = np.clip(draws, 0.0, None)\n",
    "loss_draws_mcmc = ead * lgd * (theta_draws / 100.0)\n",
    "\n",
    "loss_mean_mcmc = loss_draws_mcmc.mean()\n",
    "loss_q05_mcmc, loss_q95_mcmc = np.quantile(loss_draws_mcmc, [0.05, 0.95])\n",
    "\n",
    "print(f\"Posterior mean expected loss (MCMC): ${loss_mean_mcmc:,.0f}\")\n",
    "print(f\"90% posterior interval (MCMC): [${loss_q05_mcmc:,.0f}, ${loss_q95_mcmc:,.0f}]\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(loss_draws_mcmc / 1e6, bins=40, color='darkorange', edgecolor='white')\n",
    "plt.axvline(loss_mean_mcmc / 1e6, color='black', linestyle='--', label='Posterior mean')\n",
    "plt.title('Expected Loss Posterior (Gamma Prior)')\n",
    "plt.xlabel('Loss (million USD)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fdcfce",
   "metadata": {},
   "source": [
    "*Interpretation*: This version uses package-based MCMC (`emcee`) with a Gaussian Metropolis-Hastings move, so you can teach both non-conjugate Bayesian inference and practical sampler diagnostics with a production-grade library."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
