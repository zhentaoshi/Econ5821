{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Integration\n",
    "\n",
    "### Zhentao Shi\n",
    "\n",
    "<img src=\"graph/damaoshan.jpeg\" width=\"1000\">\n",
    "\n",
    "<!-- code is tested on SCRP -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Stochastic integration\n",
    "* Markov Chain Monte Carlo (MCMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Numerical Methods\n",
    "\n",
    "* Numerical differentiation and integration\n",
    "\n",
    "* Use case: To find the optimum for the objective function $f:R^K \\mapsto R$\n",
    "by Newton's method, \n",
    "  * $K$-dimensional gradient   \n",
    "  * $K\\times K$-dimensional Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Programming up the gradient and the Hessian manually is a time-consuming and error-prone job\n",
    "* Whenever we change the objective function, we have to redo the gradient and Hessian\n",
    "\n",
    "* More efficient to use numerical differentiation instead of the analytical expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The partial derivative of a multivariate function\n",
    "$f:R^K \\mapsto R$ at a point $x_0 \\in R^K$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x_k}\\bigg|_{x=x_0}=\\lim_{\\epsilon \\to 0}\n",
    "\\frac{f(x_0+\\epsilon \\cdot e_k) - f(x_0 - \\epsilon \\cdot e_k)}{2\\epsilon},\n",
    "$$\n",
    "\n",
    "where $e_k = (0,\\ldots,0,1,0,\\ldots,0)$ is the identifier of the $k$-th coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Numerical execution in a computer follows the basic definition to evaluate\n",
    "$f(x_0\\pm\\epsilon \\cdot e_k))$ with a small\n",
    "$\\epsilon$. \n",
    "\n",
    "* How small is small? Usually we try a sequence of $\\epsilon$'s until\n",
    "the numerical derivative is stable. \n",
    "\n",
    "* There are also more sophisticated algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Caution\n",
    "\n",
    "* Numerical methods are not panacea\n",
    "* Not all functions are differentiable or integrable.\n",
    "* Before turning to numerical methods, it is always imperative to try to understand the behavior of the function at the first place.\n",
    "* AI tools can be very helpful in these math calculation and coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Integration\n",
    "\n",
    "* An alternative to analytic/numerical integration is the stochastic methods.\n",
    "* The underlying principle of stochastic integration is the law of large numbers.\n",
    "\n",
    "* Let  $\\int h(x) d F(x)$ be an integral where $F(x)$ is a probability distribution.\n",
    "* Approximate the integral by\n",
    "$\\int h(x) d F(x) \\approx S^{-1} \\sum_{s=1}^S h(x_s)$, where $x_s$ is randomly\n",
    "generated from $F(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* When $S$ is large, a law of large numbers gives\n",
    "\n",
    "$$\n",
    "S^{-1} \\sum_{s=1}^S h(x_s) \\stackrel{\\mathrm{p}}{\\to} E[h(x)] = \\int h(x) d F(x).\n",
    "$$\n",
    "\n",
    "* If the integration is carried out not in the entire support of $F(x)$ but on a subset $A$, then\n",
    "\n",
    "$$\n",
    "\\int_A h(x) d F(x) \\approx S^{-1} \\sum_{s=1}^S h(x_s) \\cdot 1\\{x_s \\in A\\},\n",
    "$$\n",
    "\n",
    "where $1\\{\\cdot\\}$ is the indicator function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* In theory, use an $S$ as large as possible.\n",
    "* In reality, constrained by the computer's memory and computing time.\n",
    "* No clear guidance of the size of $S$ in practice. \n",
    "* Preliminary experiment can help decide an $S$ that produces stable results.\n",
    "\n",
    "* Stochastic integration is popular in econometrics and statistics, thanks to its convenience in execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example**: Use simulated maximum likelihood to estimate the parameters of a binary choice model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "random_seed = 123\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "beta_true = [0.1, 0.1]  # True coefficients\n",
    "n = 1000           # Number of observations\n",
    "k = len(beta_true)      # Number of coefficients\n",
    "\n",
    "# Generate data\n",
    "X = np.random.rand(n, k) \n",
    "X[:, 0] = 1  # Intercept\n",
    "\n",
    "error = np.random.normal(0, 1, size=n)\n",
    "y_latent = np.dot(X, beta_true) + error\n",
    "\n",
    "y = (y_latent > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nlopt\n",
    "\n",
    "# Define the simulated likelihood function\n",
    "def sim_likelihood(params, n_sims=500):\n",
    "    \"\"\"\n",
    "    Simulated likelihood function for the Probit model.\n",
    "    \"\"\"\n",
    "    beta = params\n",
    "    # Simulate the latent variable\n",
    "    error_sim = np.random.normal(0, 1, size=(n, n_sims))\n",
    "    y_latent_sim = np.tile( (X @ beta).reshape(-1, 1), n_sims) + error_sim\n",
    "    # Calculate the probability of observed y\n",
    "    prob = np.mean(y_latent_sim > 0, axis=1)\n",
    "    # Likelihood contribution\n",
    "    ll = np.sum(y * np.log(prob) + (1 - y) * np.log(1 - prob))\n",
    "    return -ll  # Minimize the negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate the parameters using SML\n",
    "# Initial guess for parameters\n",
    "beta_guess = [0.2, 0.2]\n",
    "\n",
    "# The following `minimize` function is not working with the simulated likelihood\n",
    "# results = minimize(sim_likelihood, beta_guess, method=\"BFGS\")\n",
    "\n",
    "# Define the objective function for nlopt\n",
    "def objective_function(params, grad):\n",
    "    return sim_likelihood(params)\n",
    "\n",
    "# Set up the optimizer\n",
    "opt = nlopt.opt(nlopt.LN_BOBYQA, len(beta_guess))\n",
    "opt.set_min_objective(objective_function)\n",
    "opt.set_xtol_rel(1e-6)\n",
    "\n",
    "# Run the optimization\n",
    "beta_opt = opt.optimize(beta_guess)\n",
    "minf = opt.last_optimum_value()\n",
    "\n",
    "# Print the results\n",
    "print(\"True parameters:\", beta_true)\n",
    "print(\"Estimated parameters:\", beta_opt)\n",
    "print(\"Value of the criterion function:\", minf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Variable Generation\n",
    "\n",
    "* **If the CDF $F(X)$ is known**, we can generate random variables that follow such a distribution.\n",
    "  * Draw $U$ from  $\\mathrm{Uniform}(0,1)$\n",
    "  * Compute $X = F^{-1}(U)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov Chain Monte Carlo\n",
    "\n",
    "* **If the pdf $f(X)$ is known**, we can generate a sample by *importance sampling*\n",
    "  * [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)\n",
    " (MH algorithm) is such a method.\n",
    "  * MH is one of the [Markov Chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Metropolis-Hastings Algorithm\n",
    "\n",
    "* Theory of the MH requires long derivation\n",
    "* Implementation is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Use MH to generate a sample of normally distributed observations with\n",
    "$\\mu = 1$ and $\\sigma = 0.5$.\n",
    "\n",
    "* In the function `metrop`, we provide the logarithm of the density of\n",
    "\n",
    "$$\n",
    "\\log f(x) = -\\frac{1}{2} \\log (2\\pi) - \\log \\sigma - \\frac{1}{2\\sigma^2} (x-\\mu)^2\n",
    "$$\n",
    "\n",
    "  * The first term can be omitted as it is irrelevant to the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "\n",
    "# the target distribution\n",
    "def h(x, mu=1, sd=0.5):\n",
    "    return -np.log(sd) - (x - mu)**2 / (2 * sd**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# a self-defined metropolis algorithm\n",
    "\n",
    "def metropolis(logpdf, initial, n_samples, n_spac, proposal_sd=0.5):\n",
    "# logpdf: log density function. It's a function. \n",
    "# initial: initial value\n",
    "# n_samples: total number of samples to generate\n",
    "# n_spac: number of samples to generate before saving one\n",
    "# proposal_sd: standard deviation of the normal proposal distribution\n",
    "    \n",
    "    total_iter = n_samples * n_spac\n",
    "    chain = []\n",
    "    current = initial\n",
    "    current_logpdf = logpdf(current)\n",
    "    for i in range(total_iter):\n",
    "        proposal = current + np.random.normal(0, proposal_sd) # normal proposal distribution\n",
    "        # the proposal distribution is symmetric. It determines the acceptance probability\n",
    "        proposal_logpdf = logpdf(proposal)\n",
    "        if np.log(np.random.rand()) < (proposal_logpdf - current_logpdf):\n",
    "            current = proposal\n",
    "            current_logpdf = proposal_logpdf\n",
    "        if (i + 1) % n_spac == 0:\n",
    "            chain.append(current)\n",
    "    return np.array(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(8, 12))\n",
    "\n",
    "# 1) Time series with flat steps: nbatch=100, nspac=1\n",
    "chain1 = metropolis(h, initial=0, n_samples=100, n_spac=1)\n",
    "axes[0].plot(chain1, linestyle='-', marker='o')\n",
    "axes[0].set_title(\"Time series with flat steps (nspac=1)\")\n",
    "\n",
    "# 2) Time series with reduced serial correlation: nbatch=100, nspac=10\n",
    "chain2 = metropolis(h, initial=0, n_samples=100, n_spac=10)\n",
    "axes[1].plot(chain2, linestyle='-', marker='o')\n",
    "axes[1].set_title(\"Time series looks like white noise (nspac=10)\")\n",
    "\n",
    "# 3) Density estimate versus true density: nbatch=10000, nspac=10\n",
    "chain3 = metropolis(h, initial=0, n_samples=10000, n_spac=10)\n",
    "density = gaussian_kde(chain3)\n",
    "xbase = np.linspace(-2, 2, 400)\n",
    "axes[2].plot(xbase, density(xbase), lw=2, label=\"Kernel Density\")\n",
    "axes[2].plot(xbase, norm.pdf(xbase, loc=1, scale=0.5), color='red', label=\"True Density\")\n",
    "axes[2].set_title(\"Kernel density of simulated observations\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Outcomes\n",
    "\n",
    "* The first panel is a time series where\n",
    "the marginal distribution of each observations follows $N(1,0.5^2)$. \n",
    "  * Time dependence is visible\n",
    "  * flat regions are observed when the Markov chain rejects a new proposal\n",
    "so the value does not update over two periods. \n",
    "\n",
    "* The middle panel collects the time series every 10 observations on the Markov chain\n",
    "  * serial correlation is weakened\n",
    "  * No flat region is observed\n",
    " \n",
    "  \n",
    "* The third panel compares     \n",
    "  * kernel density of the simulated observations (black curve) \n",
    "  * density function of $N(1,0.5^2)$ (red curve).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Inference\n",
    "\n",
    "\n",
    "* Bayesian framework offers a coherent and natural language for statistical decision. \n",
    "* Bayesian views both the data $\\mathbf{X}_{n}$ and the\n",
    "parameter $\\theta$ as random variables\n",
    "* Before observeing the data, researcher holds a *prior distribution* $\\pi$ about $\\theta$\n",
    "* After observing the data, researcher updates the prior distribution to a *posterior distribution* $p(\\theta|\\mathbf{X}_{n})$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "* Let $f(\\mathbf{X}_{n}|\\theta)$ be the likelihood\n",
    "* Let $\\pi$ be the prior\n",
    "\n",
    "* The celebrated Bayes Theorem is\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathbf{X}_{n})\\propto f(\\mathbf{X}_{n}|\\theta)\\pi(\\theta)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classical Analytical Example \n",
    "\n",
    "* $\\mathbf{X}_{n}=(X_{1},\\ldots,X_{n})$ is an iid sample drawn from a normal distribution with unknown $\\theta$ and known $\\sigma$\n",
    "* If a researcher's prior distribution\n",
    "$\\theta\\sim N(\\theta_{0},\\sigma_{0}^{2})$, her posterior distribution\n",
    "is, by some routine calculation, also a normal distribution\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathbf{x}_{n})\\sim N\\left(\\tilde{\\theta},\\tilde{\\sigma}^{2}\\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "$\\tilde{\\theta}=\\frac{\\sigma_{0}^{2}}{n\\sigma^{2}+\\sigma_{0}^{2}}\\theta_{0}+\\frac{n\\sigma^{2}}{n\\sigma^{2}+\\sigma_{0}^{2}}\\bar{x}$\n",
    "and\n",
    "$\\tilde{\\sigma}^{2}=\\frac{\\sigma_{0}^{2}\\sigma^{2}}{n\\sigma_{0}^{2}+\\sigma^{2}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Credible Set\n",
    "\n",
    "$$\n",
    "\\left(\\tilde{\\theta}-z_{1-\\alpha/2}\\cdot\\tilde{\\sigma},\\ \\tilde{\\theta}+z_{1-\\alpha/2}\\cdot\\tilde{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "* Posterior distribution depends on $\\theta_{0}$ and $\\sigma_{0}^{2}$\n",
    "from the prior. \n",
    "* When the sample size is sufficiently, the data overwhelms the prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Frequentist Confidence Internval\n",
    "\n",
    "\n",
    "* $\\hat{\\theta}=\\bar{x}\\sim N(\\theta,\\sigma^{2}/n)$. \n",
    "\n",
    "* Confidence interval is\n",
    "\n",
    "$$\n",
    "\\left(\\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n},\\ \\bar{x}-z_{1-\\alpha/2}\\cdot\\sigma/\\sqrt{n}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparison\n",
    "\n",
    "* Bayesian produces a posterior distribution\n",
    "  * The posterior distribution implies point estimates and credible set\n",
    "  * Data are fixed (invariant)\n",
    "  * A prior distribution is needed\n",
    "\n",
    "* Frequestist produces a point estimator\n",
    "  * Before data are observed, the point estimator is random\n",
    "  * Inference is imply by the point estimator **before observation**\n",
    "  * Only data are realized, the point estimate is a fixed number\n",
    "  * No prior distribution is needed\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The nature generates data\n",
    "* The researcher correctly specifies the normal model\n",
    "* Given the data, she infers $\\theta$ according to the posterior distribution\n",
    "\n",
    "### Code Example\n",
    "\n",
    "* Conduct inference about the mean parameter of a normal with unit variance\n",
    "* Prior distribution $\\mu \\sim Beta(2,2)$\n",
    "* [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) is flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "n = 2000\n",
    "\n",
    "# generate n random numbers from a normal distribution\n",
    "x = np.random.normal(size=n) + 0.9\n",
    "\n",
    "def loglik(theta):\n",
    "    # Computes the sum of log densities of x ~ N(theta, 1)\n",
    "    return np.sum(norm.logpdf(x, loc=theta, scale=1))\n",
    "\n",
    "def posterior(theta):\n",
    "    # Adds the log density of a Beta(2,2) prior to the log-likelihood\n",
    "    return loglik(theta) + beta.logpdf(theta, a=2, b=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nbatch = 10000\n",
    "out = metropolis(posterior, initial=0.1, n_samples=nbatch, n_spac=10)\n",
    "burn_in = round(nbatch / 10)\n",
    "out = out[burn_in:]  # remove the burn-in period\n",
    "\n",
    "# Plot the kernel density of the posterior draws\n",
    "density_est = gaussian_kde(out)\n",
    "x_grid = np.linspace(out.min(), out.max(), 200)\n",
    "\n",
    "# Print the 95% quantile interval\n",
    "quat = np.quantile(out, [0.025, 0.975])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(quat)\n",
    "\n",
    "# plot the two values in quat as vertical lines\n",
    "plt.figure()\n",
    "plt.plot(x_grid, density_est(x_grid))\n",
    "\n",
    "plt.axvline(quat[0], color='red', linestyle='--')\n",
    "plt.axvline(quat[1], color='red', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
